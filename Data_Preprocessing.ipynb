{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_json = pd.read_json('./dataset/tweets_raw.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ALM</td>\n",
       "      <td>[{'tweet_id': '521033092132503552', 'tweet_tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>[{'tweet_id': '593899776564944897', 'tweet_tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BLM</td>\n",
       "      <td>[{'tweet_id': '734202176684298240', 'tweet_tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Davidson</td>\n",
       "      <td>[{'tweet_id': '5', 'annotations': [{'annotator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Election</td>\n",
       "      <td>[{'tweet_id': '509464992404357120', 'tweet_tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>MeToo</td>\n",
       "      <td>[{'tweet_id': '48430014437122048', 'tweet_text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>[{'tweet_id': '258018822945120256', 'tweet_tex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Corpus                                             Tweets\n",
       "0        ALM  [{'tweet_id': '521033092132503552', 'tweet_tex...\n",
       "1  Baltimore  [{'tweet_id': '593899776564944897', 'tweet_tex...\n",
       "2        BLM  [{'tweet_id': '734202176684298240', 'tweet_tex...\n",
       "3   Davidson  [{'tweet_id': '5', 'annotations': [{'annotator...\n",
       "4   Election  [{'tweet_id': '509464992404357120', 'tweet_tex...\n",
       "5      MeToo  [{'tweet_id': '48430014437122048', 'tweet_text...\n",
       "6      Sandy  [{'tweet_id': '258018822945120256', 'tweet_tex..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweet: 34987 \n",
      "total tweet text: 30114 \n",
      "valid tweet text [3243, 3942, 3960, 0, 4147, 0, 3673]\n"
     ]
    }
   ],
   "source": [
    "#Validity Checker\n",
    "\n",
    "sum = 0 \n",
    "total_tweet = 0\n",
    "v = []\n",
    "moral_category = {}\n",
    "\n",
    "for corpus_i in range(len(annotations_json['Corpus'])):\n",
    "    valid = 0 \n",
    "    for tweet in annotations_json['Tweets'][corpus_i]:\n",
    "        total_tweet += 1\n",
    "        if 'tweet_text' in tweet:\n",
    "            sum += 1 \n",
    "            if tweet['tweet_text'] != 'no tweet text available':\n",
    "                valid += 1\n",
    "        for annotation in tweet['annotations']:\n",
    "            annotations = annotation['annotation'].split(',')\n",
    "            for i in annotations:\n",
    "                if i not in moral_category:\n",
    "                    moral_category[i] = 0\n",
    "                else:\n",
    "                    moral_category[i] += 1\n",
    "    v.append(valid)\n",
    "                \n",
    "print(\"total tweet:\",total_tweet,\"\\ntotal tweet text:\",sum,\"\\nvalid tweet text\",v)\n",
    "            \n",
    "\n",
    "#Setting up global variable\n",
    "moral_OHL = {}\n",
    "moral_OHL['care'] = 0\n",
    "moral_OHL['purity'] = 1\n",
    "moral_OHL['subversion'] = 2 \n",
    "moral_OHL['loyalty'] = 3 \n",
    "moral_OHL['harm'] = 4\n",
    "moral_OHL['cheating'] = 5 \n",
    "moral_OHL['fairness'] = 6 \n",
    "moral_OHL['non-moral'] = 7 \n",
    "moral_OHL['betrayal'] = 8 \n",
    "moral_OHL['authority'] = 9 \n",
    "moral_OHL['degradation'] =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets and annotations for each individual annotator\n",
    "\n",
    "# DATA-SET1 Individual corpus with individual annotator \n",
    "# DATA-SET2 Individual corpus with mix annotator \n",
    "# DATA-SET3 Mix corpus with mix annotator \n",
    "# DATA-SET4 Mix corpus with Individual annotator \n",
    "\n",
    "import re\n",
    "\n",
    "annotators_tweets = {}\n",
    "\n",
    "for corpus_i in range(len(annotations_json['Corpus'])):\n",
    "    for tweet in annotations_json['Tweets'][corpus_i]:\n",
    "        for annotation in tweet['annotations']:\n",
    "            \n",
    "            # make sure annotator exists in the dictionary\n",
    "            if annotation['annotator'] not in annotators_tweets:\n",
    "                annotators_tweets[annotation['annotator']] = []\n",
    "                \n",
    "            if 'tweet_text' in tweet:\n",
    "                if tweet['tweet_text'] != 'no tweet text available':\n",
    "                    \n",
    "                    #Step 1 Basic Clean up\n",
    "                    text = re.sub(r'http\\S+', '', tweet['tweet_text'])\n",
    "                    \n",
    "                    #Getting rid off @\n",
    "                    text = re.sub(r'@\\S+', '', text)\n",
    "                    \n",
    "                    #Getting rid off hashtag\n",
    "                    text = re.sub(r'#\\S+', '', text)\n",
    "                    \n",
    "                    #Getting rid off &\n",
    "                    text = re.sub(r'&amp', '', text)\n",
    "            \n",
    "                    #Step 2 - Lowercasing \n",
    "                    text = text.lower()\n",
    "                    \n",
    "                    #Step 3 - Tokenization\n",
    "                    text = word_tokenize(text)\n",
    "\n",
    "                    #Step 4 - Stopwords removal \n",
    "                    stop_words = set(stopwords.words('english')) \n",
    "                    text = [i for i in text if not i in stop_words]\n",
    "                    \n",
    "                    #Step 5 - Stemming\n",
    "                    newtext = []\n",
    "                    for i in text:\n",
    "                        newtext.append(ps.stem(i))\n",
    "                        \n",
    "                    #Step 6 - lemmatization\n",
    "                    new = []\n",
    "                    for i in newtext:\n",
    "                        new.append(lemmatizer.lemmatize(i))\n",
    "                        \n",
    "                    text = \" \".join(new)\n",
    "  \n",
    "                    #Setting up onehot for annotations. \n",
    "                    annotations = annotation['annotation'].split(',')\n",
    "                    \n",
    "                    moral_labels = [0 for i in range(11)]\n",
    "                    \n",
    "                    for i in annotations:\n",
    "                        if i in moral_OHL:\n",
    "                            moral_labels[moral_OHL[i]] = 1\n",
    "            \n",
    "                    # arrange data format of each tweet \n",
    "                    new_tweet = { 'tweet': text,\n",
    "                                 'moral labels': moral_labels,\n",
    "                                 'corpus': annotations_json['Corpus'][corpus_i],\n",
    "                                'tweet_id': tweet['tweet_id'],\n",
    "                                'annotations': annotations, \n",
    "                                }\n",
    "                    annotators_tweets[annotation['annotator']].append(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tweet': 'wholeheartedli support protest ; act civil disobedi ; join !',\n",
       "  'moral labels': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '537681598989475841',\n",
       "  'annotations': ['loyalty']},\n",
       " {'tweet': 'sandra bland situat man disrespect rest soul , peopl die everyday unjustifi matter',\n",
       "  'moral labels': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '624644420705648640',\n",
       "  'annotations': ['cheating']},\n",
       " {'tweet': 'commit peac , heal love neighbor . give u strength patienc .',\n",
       "  'moral labels': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '752979765984890884',\n",
       "  'annotations': ['care', 'purity']},\n",
       " {'tweet': 'injustic one injustic',\n",
       "  'moral labels': [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '548029362348765185',\n",
       "  'annotations': ['care', 'loyalty', 'purity']},\n",
       " {'tweet': 'compass look like !',\n",
       "  'moral labels': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '753261224737898497',\n",
       "  'annotations': ['care', 'purity']},\n",
       " {'tweet': 'liberti justic ? opportun .',\n",
       "  'moral labels': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '636902749657919489',\n",
       "  'annotations': ['care', 'purity']},\n",
       " {'tweet': 'took long time , ? doctor strive le harm inattent care , via',\n",
       "  'moral labels': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '568112472542064640',\n",
       "  'annotations': ['non-moral']},\n",
       " {'tweet': \"ye rt ppl chang run thru cancer fundrais goin '' diseas ''\",\n",
       "  'moral labels': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '540400187458351105',\n",
       "  'annotations': ['cheating']},\n",
       " {'tweet': '... give random act kind today ! ! !',\n",
       "  'moral labels': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '751417943427149824',\n",
       "  'annotations': ['care', 'purity']},\n",
       " {'tweet': 'racist behind prefer crimin innoc',\n",
       "  'moral labels': [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "  'corpus': 'ALM',\n",
       "  'tweet_id': '546194676441546752',\n",
       "  'annotations': ['cheating', 'betrayal']}]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize annotator01's first 5 tweets with annotations\n",
    "annotators_tweets['annotator02'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotator01 , annotated amount: 3106\n",
      "annotator02 , annotated amount: 3213\n",
      "annotator03 , annotated amount: 3234\n"
     ]
    }
   ],
   "source": [
    "#13 annotators \n",
    "\n",
    "data_tweets = {}\n",
    "\n",
    "for annotator in annotators_tweets:\n",
    "    if len(annotators_tweets[annotator]) > 500:\n",
    "        data_tweets[annotator] = annotators_tweets[annotator]\n",
    "        print(annotator, ', annotated amount:', len(data_tweets[annotator]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA-SET1  Individual annotator with Individual corpus with\n",
    "# DATA-SET2  Individual annotator with mixed corpus \n",
    "# DATA-SET3  Mixed annotator with individual corpus  \n",
    "# DATA-SET4  Mixed annotator with mixed corpus \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This Sandra Bland situation man no disrespect rest her soul , but people die everyday in a unjustified matter #AllLivesMatter', 'Commitment to peace, healing and loving neighbors. Give us strength and patience. #PortlandPride #AllLivesMatter #Peace', 'Injustice for one is an injustice for all #AllLivesMatter  #AntonioMartin', 'This is what compassion looks like! #vegan #AllLivesMatter ', 'Liberty and Justice for all? How about opportunity for all. #blacklivesmatter #alllivesmatter']\n",
      "[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# DATA-SET1 Individual annotator with Individual corpus with\n",
    "\n",
    "data_set1 = {}\n",
    "\n",
    "for annotator in data_tweets:\n",
    "    if annotator not in data_set1:\n",
    "        data_set1[annotator] = {}\n",
    "    for i in data_tweets[annotator]:\n",
    "        if i['corpus'] not in data_set1[annotator]:\n",
    "            data_set1[annotator][i['corpus']] = [[],[]]\n",
    "        else:\n",
    "            data_set1[annotator][i['corpus']][0].append(i['tweet'])\n",
    "            data_set1[annotator][i['corpus']][1].append(i['moral labels'])\n",
    "                \n",
    "\n",
    "print(data_set1['annotator02']['ALM'][0][:5])\n",
    "print(data_set1['annotator02']['ALM'][1][:5])\n",
    "\n",
    "with open('./dataset/iaic.json', 'w') as outfile:\n",
    "    json.dump(data_set1, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wholeheartedly support these protests ; acts of civil disobedience ; will join when I can! #Ferguson #AllLivesMatter ', 'This Sandra Bland situation man no disrespect rest her soul , but people die everyday in a unjustified matter #AllLivesMatter', 'Commitment to peace, healing and loving neighbors. Give us strength and patience. #PortlandPride #AllLivesMatter #Peace', 'Injustice for one is an injustice for all #AllLivesMatter  #AntonioMartin', 'This is what compassion looks like! #vegan #AllLivesMatter ']\n",
      "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# DATA-SET2 Individual annotator with mixed corpus \n",
    "\n",
    "data_set2 = {}\n",
    "\n",
    "for annotator in data_tweets:\n",
    "    if annotator not in data_set2:\n",
    "        data_set2[annotator] = [[],[]]\n",
    "    for i in data_tweets[annotator]:\n",
    "        data_set2[annotator][0].append(i['tweet'])\n",
    "        data_set2[annotator][1].append(i['moral labels'])\n",
    "                \n",
    "\n",
    "print(data_set2['annotator02'][0][:5])\n",
    "print(data_set2['annotator02'][1][:5])\n",
    "\n",
    "with open('./dataset/iamc.json', 'w') as outfile:\n",
    "    json.dump(data_set2, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This Sandra Bland situation man no disrespect rest her soul , but people die everyday in a unjustified matter #AllLivesMatter', 'Commitment to peace, healing and loving neighbors. Give us strength and patience. #PortlandPride #AllLivesMatter #Peace', 'Injustice for one is an injustice for all #AllLivesMatter  #AntonioMartin', 'This is what compassion looks like! #vegan #AllLivesMatter ', 'Black Twitter when they see someone tweet #AllLivesMatter ']\n",
      "[[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# DATA-SET3  Mixed annotator with individual corpus  \n",
    "\n",
    "data_set3 = {}\n",
    "\n",
    "for annotator in data_tweets:\n",
    "    for i in data_tweets[annotator]:\n",
    "        if i['corpus'] not in data_set3:\n",
    "            data_set3[i['corpus']] = [[],[]]\n",
    "        else:\n",
    "            data_set3[i['corpus']][0].append(i['tweet'])\n",
    "            data_set3[i['corpus']][1].append(i['moral labels'])\n",
    "                \n",
    "print(data_set3['ALM'][0][:5])\n",
    "print(data_set3['ALM'][1][:5])\n",
    "\n",
    "with open('./dataset/maic.json', 'w') as outfile:\n",
    "    json.dump(data_set3, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wholeheartedly support these protests ; acts of civil disobedience ; will join when I can! #Ferguson #AllLivesMatter ', 'This Sandra Bland situation man no disrespect rest her soul , but people die everyday in a unjustified matter #AllLivesMatter', 'Commitment to peace, healing and loving neighbors. Give us strength and patience. #PortlandPride #AllLivesMatter #Peace', 'Injustice for one is an injustice for all #AllLivesMatter  #AntonioMartin', 'This is what compassion looks like! #vegan #AllLivesMatter ']\n",
      "[[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# DATA-SET4  Mixed corpus with mixed annotator \n",
    "\n",
    "data_set4 = [[],[]]\n",
    "\n",
    "for annotator in data_tweets:\n",
    "    for i in data_tweets[annotator]:\n",
    "        data_set4[0].append(i['tweet'])\n",
    "        data_set4[1].append(i['moral labels'])\n",
    "                \n",
    "print(data_set4[0][:5])\n",
    "print(data_set4[1][:5])\n",
    "\n",
    "with open('./dataset/mamc.json', 'w') as outfile:\n",
    "    json.dump(data_set4, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1  2  3'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['1','2','3']\n",
    "b = '  '.join(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run toward tree'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"running towards a tree #hunt #worldwide #alllivesmatter \"\n",
    "\n",
    "#Getting rid off @\n",
    "text = re.sub(r'@\\S+', '', text)\n",
    "\n",
    "#Getting rid off hashtag\n",
    "text = re.sub(r'#\\S+', '', text)\n",
    "\n",
    "#Getting rid off &\n",
    "text = re.sub(r'&amp', '', text)\n",
    "\n",
    "#Getting rid off emoji, new lines\n",
    "text = re.sub(r'\\\\\\\\u....', '', text)\n",
    "\n",
    "#Step 2 - Lowercasing \n",
    "text = text.lower()\n",
    "\n",
    "#Step 3 - Tokenization\n",
    "text = word_tokenize(text)\n",
    "\n",
    "#Step 3 - Stopwords removal \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "text = [i for i in text if not i in stop_words]\n",
    "\n",
    " \n",
    "#Step 5 - Stemming\n",
    "newtext = []\n",
    "for i in text:\n",
    "    newtext.append(ps.stem(i))\n",
    "\n",
    "#Step 6 - lemmatization\n",
    "new = []\n",
    "for i in newtext:\n",
    "    new.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "    \n",
    "new = \" \".join(new)\n",
    "\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

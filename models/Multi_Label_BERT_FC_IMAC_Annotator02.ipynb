{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-Label-BERT_FC-IMAC-Annotator02.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PbrIqkb7q0ny",
        "m6X5lLSVrEkj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbrIqkb7q0ny",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU7fYjAllVwQ",
        "colab_type": "code",
        "outputId": "1e6799ee-94ca-4c88-db3f-8a0194869cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# install transformers package from Hugging Face transformers package contains \n",
        "# pre-trained BERT model and other useful interfaces\n",
        "!pip install transformers "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8_mQkgPgasW",
        "colab_type": "code",
        "outputId": "97bfb9fa-1e99-4e20-fbea-d6f2f6ba10d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Check if Colab's GPU is available and set up the GPU device\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('GPU is available')\n",
        "else:\n",
        "    raise SystemError('No GPU device available')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU name and type:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using CPU')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n",
            "GPU name and type: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6X5lLSVrEkj",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQiDv4rylYkw",
        "colab_type": "code",
        "outputId": "37aabaa2-db3a-4f5a-da62-2c8bcfb40aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# dataset pre-processing\n",
        "# load data_extraction.ipynb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "iamc_ds = pd.read_json('iamc.json')\n",
        "\n",
        "# Testing with annotator02\n",
        "tweets = np.array(list(iamc_ds['annotator02'][0]))\n",
        "labels = np.array(list(iamc_ds['annotator02'][1]))\n",
        "\n",
        "print('example 1')\n",
        "print('tweet: ', tweets[0])\n",
        "print('label: ', labels[0])\n",
        "\n",
        "print('')\n",
        "print('example 2')\n",
        "print('tweet: ', tweets[1])\n",
        "print('label: ', labels[1])\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example 1\n",
            "tweet:  Wholeheartedly support these protests ; acts of civil disobedience ; will join when I can! #Ferguson #AllLivesMatter \n",
            "label:  [0 0 0 1 0 0 0 0 0 0 0]\n",
            "\n",
            "example 2\n",
            "tweet:  This Sandra Bland situation man no disrespect rest her soul , but people die everyday in a unjustified matter #AllLivesMatter\n",
            "label:  [0 0 0 0 0 1 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qFUYwuZrd4e",
        "colab_type": "text"
      },
      "source": [
        "# Training Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omQcPA7UrNVm",
        "colab_type": "text"
      },
      "source": [
        "### Input Tokenization & Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfQNYUK2sGRO",
        "colab_type": "code",
        "outputId": "a1c263f4-a141-45b1-bf5d-6bfb7cbd5663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# visualize BERT tokenization by an example\n",
        "print('Before:', tweets[0])\n",
        "print('After(words):', tokenizer.tokenize(tweets[0]))\n",
        "print('After(ids):', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n",
            "Before: Wholeheartedly support these protests ; acts of civil disobedience ; will join when I can! #Ferguson #AllLivesMatter \n",
            "After(words): ['whole', '##hearted', '##ly', 'support', 'these', 'protests', ';', 'acts', 'of', 'civil', 'di', '##so', '##bed', '##ience', ';', 'will', 'join', 'when', 'i', 'can', '!', '#', 'ferguson', '#', 'all', '##li', '##ves', '##mat', '##ter']\n",
            "After(ids): [2878, 27693, 2135, 2490, 2122, 8090, 1025, 4490, 1997, 2942, 4487, 6499, 8270, 13684, 1025, 2097, 3693, 2043, 1045, 2064, 999, 1001, 11262, 1001, 2035, 3669, 6961, 18900, 3334]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77-MUinHvdk9",
        "colab_type": "code",
        "outputId": "8efeb76d-55c5-4584-d78b-fceb8abf6db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Get the maximum length of the dataset since all input instances have to have a constant length\n",
        "max_len = 0\n",
        "\n",
        "for tweet in tweets:\n",
        "    max_len = max(max_len, len(tweet.split()))\n",
        "\n",
        "max_len += 2\n",
        "print('Max tweet length:', max_len)\n",
        "\n",
        "\n",
        "# tokenize all tweets, acquire corresponding token ids and attention masks\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for tweet in tweets:\n",
        "    encoded_dict = tokenizer.encode_plus(tweet, \n",
        "                                         add_special_tokens=True, \n",
        "                                         max_length=max_len,\n",
        "                                         pad_to_max_length=True,\n",
        "                                         return_attention_mask=True,\n",
        "                                         return_tensors='pt'\n",
        "                                         )\n",
        "    \n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "# visualize tweet, tokenized ids and attention masks\n",
        "print('Original tweet: ', tweets[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Attention Mask:', attention_masks[0])\n",
        "print('Labels:', labels[0])\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-21f78d223e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LCo4ZmmrnnA",
        "colab_type": "text"
      },
      "source": [
        "### Training & Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K9bBXNtsIqf",
        "colab_type": "code",
        "outputId": "60608787-ea51-4a9e-e919-afb0ea45c625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# training 80% validation 10% testing 10%\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "print('training dataset size:', train_size)\n",
        "print('validation dataset size:', val_size)\n",
        "print('testing dataset size:', test_size)\n",
        "\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# create a dataloader\n",
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(train_ds, \n",
        "                      sampler=RandomSampler(train_ds), \n",
        "                      batch_size=batch_size)\n",
        "\n",
        "val_dl = DataLoader(val_ds, \n",
        "                    sampler=SequentialSampler(val_ds), \n",
        "                    batch_size=batch_size)\n",
        "\n",
        "test_dl = DataLoader(test_ds, \n",
        "                    sampler=SequentialSampler(test_ds), \n",
        "                    batch_size=batch_size)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training dataset size: 11582\n",
            "validation dataset size: 1447\n",
            "testing dataset size: 1449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7wj3u8FVdK_",
        "colab_type": "text"
      },
      "source": [
        "### Building Multi-Label BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlviX-FpVvVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel, BertConfig, BertPreTrainedModel, BertForSequenceClassification\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
        "    \"\"\" \n",
        "    Bert for multi-label classification \n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels=11):\n",
        "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
        "    \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            new_loss = BCEWithLogitsLoss()\n",
        "            loss = new_loss(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSRiQzlJr-Sx",
        "colab_type": "text"
      },
      "source": [
        "### Model Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxaFYgK0rnSP",
        "colab_type": "code",
        "outputId": "3d19f3b8-0422-42c0-e37d-78f1775b75f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForMultiLabelSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
        "                                                                 output_attentions=False, \n",
        "                                                                 output_hidden_states=False)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "#visualize\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "for p in params:\n",
        "    print(\"{:<60} {:>15}\".format(p[0], str(tuple(p[1].size()))))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert.embeddings.word_embeddings.weight                          (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                        (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                        (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                                      (768,)\n",
            "bert.embeddings.LayerNorm.bias                                        (768,)\n",
            "bert.encoder.layer.0.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.1.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.1.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.1.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.1.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.1.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.1.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.1.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.1.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.1.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.1.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.1.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.1.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.1.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.1.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.2.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.2.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.2.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.2.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.2.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.2.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.2.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.2.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.2.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.2.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.2.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.2.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.2.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.2.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.3.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.3.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.3.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.3.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.3.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.3.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.3.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.3.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.3.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.3.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.3.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.3.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.3.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.3.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.4.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.4.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.4.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.4.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.4.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.4.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.4.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.4.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.4.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.4.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.4.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.4.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.4.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.4.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.5.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.5.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.5.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.5.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.5.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.5.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.5.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.5.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.5.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.5.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.5.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.5.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.5.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.5.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.6.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.6.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.6.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.6.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.6.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.6.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.6.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.6.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.6.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.6.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.6.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.6.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.6.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.6.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.7.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.7.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.7.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.7.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.7.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.7.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.7.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.7.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.7.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.7.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.7.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.7.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.7.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.7.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.8.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.8.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.8.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.8.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.8.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.8.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.8.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.8.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.8.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.8.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.8.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.8.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.8.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.8.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.9.attention.self.query.weight                  (768, 768)\n",
            "bert.encoder.layer.9.attention.self.query.bias                        (768,)\n",
            "bert.encoder.layer.9.attention.self.key.weight                    (768, 768)\n",
            "bert.encoder.layer.9.attention.self.key.bias                          (768,)\n",
            "bert.encoder.layer.9.attention.self.value.weight                  (768, 768)\n",
            "bert.encoder.layer.9.attention.self.value.bias                        (768,)\n",
            "bert.encoder.layer.9.attention.output.dense.weight                (768, 768)\n",
            "bert.encoder.layer.9.attention.output.dense.bias                      (768,)\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight                (768,)\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias                  (768,)\n",
            "bert.encoder.layer.9.intermediate.dense.weight                   (3072, 768)\n",
            "bert.encoder.layer.9.intermediate.dense.bias                         (3072,)\n",
            "bert.encoder.layer.9.output.dense.weight                         (768, 3072)\n",
            "bert.encoder.layer.9.output.dense.bias                                (768,)\n",
            "bert.encoder.layer.9.output.LayerNorm.weight                          (768,)\n",
            "bert.encoder.layer.9.output.LayerNorm.bias                            (768,)\n",
            "bert.encoder.layer.10.attention.self.query.weight                 (768, 768)\n",
            "bert.encoder.layer.10.attention.self.query.bias                       (768,)\n",
            "bert.encoder.layer.10.attention.self.key.weight                   (768, 768)\n",
            "bert.encoder.layer.10.attention.self.key.bias                         (768,)\n",
            "bert.encoder.layer.10.attention.self.value.weight                 (768, 768)\n",
            "bert.encoder.layer.10.attention.self.value.bias                       (768,)\n",
            "bert.encoder.layer.10.attention.output.dense.weight               (768, 768)\n",
            "bert.encoder.layer.10.attention.output.dense.bias                     (768,)\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight               (768,)\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias                 (768,)\n",
            "bert.encoder.layer.10.intermediate.dense.weight                  (3072, 768)\n",
            "bert.encoder.layer.10.intermediate.dense.bias                        (3072,)\n",
            "bert.encoder.layer.10.output.dense.weight                        (768, 3072)\n",
            "bert.encoder.layer.10.output.dense.bias                               (768,)\n",
            "bert.encoder.layer.10.output.LayerNorm.weight                         (768,)\n",
            "bert.encoder.layer.10.output.LayerNorm.bias                           (768,)\n",
            "bert.encoder.layer.11.attention.self.query.weight                 (768, 768)\n",
            "bert.encoder.layer.11.attention.self.query.bias                       (768,)\n",
            "bert.encoder.layer.11.attention.self.key.weight                   (768, 768)\n",
            "bert.encoder.layer.11.attention.self.key.bias                         (768,)\n",
            "bert.encoder.layer.11.attention.self.value.weight                 (768, 768)\n",
            "bert.encoder.layer.11.attention.self.value.bias                       (768,)\n",
            "bert.encoder.layer.11.attention.output.dense.weight               (768, 768)\n",
            "bert.encoder.layer.11.attention.output.dense.bias                     (768,)\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight               (768,)\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias                 (768,)\n",
            "bert.encoder.layer.11.intermediate.dense.weight                  (3072, 768)\n",
            "bert.encoder.layer.11.intermediate.dense.bias                        (3072,)\n",
            "bert.encoder.layer.11.output.dense.weight                        (768, 3072)\n",
            "bert.encoder.layer.11.output.dense.bias                               (768,)\n",
            "bert.encoder.layer.11.output.LayerNorm.weight                         (768,)\n",
            "bert.encoder.layer.11.output.LayerNorm.bias                           (768,)\n",
            "bert.pooler.dense.weight                                          (768, 768)\n",
            "bert.pooler.dense.bias                                                (768,)\n",
            "classifier.weight                                                  (11, 768)\n",
            "classifier.bias                                                        (11,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpcl1aBYaD6y",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwbBN5C3aJxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "\n",
        "# set up the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=2e-5,\n",
        "                  eps=1e-8)\n",
        "\n",
        "# set up the lr scheduler\n",
        "epochs = 3\n",
        "total_steps = len(train_dl) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVHPS3_nbOM-",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkoALr0NbZjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "def get_accuracy_hamming_loss(preds, labels):\n",
        "    preds = preds >= 0\n",
        "    \n",
        "    total = []\n",
        "    for i in range(len(preds)):\n",
        "        total.append(1 - hamming_loss(preds[i],labels[i]))\n",
        "\n",
        "    return np.mean(total)\n",
        "                \n",
        "# calculate accuracy\n",
        "def get_accuracy(preds, labels):\n",
        "    preds = preds >= 0\n",
        "    \n",
        "    total = []\n",
        "    for i in range(len(preds)):\n",
        "        suM = 0\n",
        "        for j in range(len(preds[i])):\n",
        "            correct = 0\n",
        "            #correct\n",
        "            if preds[i][j] == 1 and labels[i][j]== 1 :\n",
        "                suM += 1\n",
        "                correct += 1\n",
        "            #missed \n",
        "            elif labels[i][j]== 1 and preds[i][j] == 0:\n",
        "                suM += 1\n",
        "            elif labels[i][j]== 0 and preds[i][j] == 1:\n",
        "                suM += 1\n",
        "        total.append(correct/suM)\n",
        "\n",
        "    return np.mean(total)\n",
        "                \n",
        "    # return np.mean(np.array(preds) == np.array(labels))\n",
        "\n",
        "\n",
        "# take a time in seconds and returns a string hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MYO6-Fsr2cJ",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtvWruZIZSRC",
        "colab_type": "code",
        "outputId": "da10ee6d-c9ce-4afe-e639-695f678e4037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 1\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time() \n",
        "\n",
        "# initialize BERT weights\n",
        "model.init_weights()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('======= Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print(\"\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dl):\n",
        "        # periodically update elapsed time\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            print('Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, \n",
        "                                                                 len(train_dl), \n",
        "                                                                 elapsed))\n",
        "        # unpack current batch's input & labels   \n",
        "        cur_input_ids = batch[0].to(device)\n",
        "        cur_input_mask = batch[1].to(device)\n",
        "        cur_labels = batch[2].to(device)\n",
        "\n",
        "        # clear previously calculated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # perform a forward pass\n",
        "        # logits = classifications, before activation function e.x. softmax\n",
        "        loss, logits = model(cur_input_ids, \n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=cur_input_mask,\n",
        "                             labels=cur_labels)\n",
        "        \n",
        "        # accumulate training loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # perform a backward pass to calculate the gradients of params\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the gradients if it is not in [-1,1]\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # update lr\n",
        "        scheduler.step()\n",
        "\n",
        "    # calculate stats after 1 epoch of training\n",
        "    avg_train_loss = total_train_loss / len(train_dl)\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    # validation \n",
        "    print(\"\")\n",
        "    print(\"Validating...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in val_dl:\n",
        "        \n",
        "        # unpack current batch's input and labels\n",
        "        cur_input_ids = batch[0].to(device)\n",
        "        cur_input_mask = batch[1].to(device)\n",
        "        cur_labels = batch[2].to(device)\n",
        "\n",
        "        # no need to calculate and trace gradient\n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(cur_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=cur_input_mask,\n",
        "                                   labels=cur_labels)\n",
        "                    \n",
        "        # no need to calculate prediction & labels in gpu\n",
        "        # good practice when involves with large-scale dataset\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = cur_labels.to('cpu')\n",
        "        \n",
        "        # calculate this batch's accuracy & loss and accumulate with other \n",
        "        # batches' accuracies & loss\n",
        "        total_eval_accuracy += get_accuracy(logits, label_ids)\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "    # average out loss and accuracy across all batches\n",
        "    avg_val_accuracy = total_eval_accuracy / len(val_dl)\n",
        "    avg_val_loss = total_eval_loss / len(val_dl)\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\n",
        "        \"Average Accuracy: {0:}, Loss: {1:.2f}, Time elapsed: {2:}\".format(\n",
        "            avg_val_accuracy, avg_val_loss, validation_time\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # record all statistics for this epoch\n",
        "    training_stats.append({\n",
        "        'epoch': epoch_i + 1,\n",
        "        'Training Loss': avg_train_loss,\n",
        "        'Valid. Loss': avg_val_loss,\n",
        "        'Valid. Accur': avg_val_accuracy,\n",
        "        'Training Time': training_time,\n",
        "        'Validation Time': validation_time\n",
        "    })\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training Completed. Training took {:}\".format(format_time(time.time() - total_t0)))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======= Epoch 1 / 3 ========\n",
            "\n",
            "Batch    40 of   362. Elapsed: 0:00:05.\n",
            "Batch    80 of   362. Elapsed: 0:00:11.\n",
            "Batch   120 of   362. Elapsed: 0:00:16.\n",
            "Batch   160 of   362. Elapsed: 0:00:22.\n",
            "Batch   200 of   362. Elapsed: 0:00:27.\n",
            "Batch   240 of   362. Elapsed: 0:00:32.\n",
            "Batch   280 of   362. Elapsed: 0:00:38.\n",
            "Batch   320 of   362. Elapsed: 0:00:43.\n",
            "Batch   360 of   362. Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:00:49\n",
            "\n",
            "Validating...\n",
            "Average Accuracy: 0.9124258893280632, Loss: 0.25, Time elapsed: 0:00:02\n",
            "\n",
            "======= Epoch 2 / 3 ========\n",
            "\n",
            "Batch    40 of   362. Elapsed: 0:00:05.\n",
            "Batch    80 of   362. Elapsed: 0:00:11.\n",
            "Batch   120 of   362. Elapsed: 0:00:16.\n",
            "Batch   160 of   362. Elapsed: 0:00:22.\n",
            "Batch   200 of   362. Elapsed: 0:00:27.\n",
            "Batch   240 of   362. Elapsed: 0:00:32.\n",
            "Batch   280 of   362. Elapsed: 0:00:38.\n",
            "Batch   320 of   362. Elapsed: 0:00:43.\n",
            "Batch   360 of   362. Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epcoh took: 0:00:49\n",
            "\n",
            "Validating...\n",
            "Average Accuracy: 0.9140669113495201, Loss: 0.24, Time elapsed: 0:00:02\n",
            "\n",
            "======= Epoch 3 / 3 ========\n",
            "\n",
            "Batch    40 of   362. Elapsed: 0:00:05.\n",
            "Batch    80 of   362. Elapsed: 0:00:11.\n",
            "Batch   120 of   362. Elapsed: 0:00:16.\n",
            "Batch   160 of   362. Elapsed: 0:00:22.\n",
            "Batch   200 of   362. Elapsed: 0:00:27.\n",
            "Batch   240 of   362. Elapsed: 0:00:32.\n",
            "Batch   280 of   362. Elapsed: 0:00:38.\n",
            "Batch   320 of   362. Elapsed: 0:00:43.\n",
            "Batch   360 of   362. Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epcoh took: 0:00:49\n",
            "\n",
            "Validating...\n",
            "Average Accuracy: 0.9136345990965554, Loss: 0.23, Time elapsed: 0:00:02\n",
            "\n",
            "Training Completed. Training took 0:02:32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xpcd9gUwjqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b5e3fdec-e255-4448-9db8-c56d2cde1dd1"
      },
      "source": [
        "# Display training stats\n",
        "# Display numbers with two decimal\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "df_stats"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.29</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0:00:48</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.26</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0:00:48</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0:00:48</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0:00:48</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0:00:48</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur Training Time Validation Time\n",
              "epoch                                                                        \n",
              "1               0.29         0.25          0.61       0:00:48         0:00:02\n",
              "2               0.26         0.24          0.62       0:00:48         0:00:02\n",
              "3               0.25         0.23          0.56       0:00:48         0:00:02\n",
              "4               0.23         0.23          0.53       0:00:48         0:00:02\n",
              "5               0.21         0.22          0.57       0:00:48         0:00:02"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0urcVvr-1Gl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "bb961c20-a231-413f-f8fa-1f52d9daf339"
      },
      "source": [
        "# Visualize training stats\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "### TO_DO: change epoch number\n",
        "plt.xticks([1,2,3,4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 62\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeVxUZfs/8M8MMCwCsoOCuCGDsgka\npJIKImDijnsuIW5lWj6V+rim3x5NLdQWcy33DUFFcQuXshQEDFMBE00FFBCRTdmc+f3hj8lxQEGB\nw/J5v169ivvc55xrxju85p7r3LdILpfLQURERERE9YJY6ACIiIiIiKjymMATEREREdUjTOCJiIiI\niOoRJvBERERERPUIE3giIiIionqECTwRERERUT3CBJ6IGr2UlBRIpVJ8++23r32N2bNnQyqVVmNU\nDVdF77dUKsXs2bMrdY1vv/0WUqkUKSkp1R5faGgopFIpoqKiqv3aRETVQV3oAIiIXlSVRDgyMhJW\nVlY1GE398/jxY/z444+IiIhARkYGjIyM0KlTJ3zwwQdo27Ztpa4xffp0HD9+HAcOHED79u3L7SOX\ny9GrVy/k5ubi3Llz0NLSqs6XUaOioqIQHR2NcePGQV9fX+hwVKSkpKBXr14YPXo0FixYIHQ4RFTH\nMIEnojpn+fLlSj/HxsZiz549GD58ODp16qR0zMjI6I3vZ2lpicuXL0NNTe21r7FkyRJ88cUXbxxL\ndZg3bx6OHDkCf39/uLm5ITMzE6dOnUJ8fHylE/iAgAAcP34c+/fvx7x588rtc+HCBaSmpmL48OHV\nkrxfvnwZYnHtfDEcHR2N7777DoMGDVJJ4AcMGIC+fftCQ0OjVmIhIqoqJvBEVOcMGDBA6eenT59i\nz5496Nixo8qxF+Xn50NXV7dK9xOJRNDU1KxynM+rK8nekydPcOzYMXh4eODrr79WtE+bNg3FxcWV\nvo6HhweaNWuG8PBwfP7555BIJCp9QkNDATxL9qvDm/4ZVBc1NbU3+jBHRFTTWANPRPWWl5cXxowZ\ng2vXrmHChAno1KkT+vfvD+BZIh8cHIyhQ4fC3d0dDg4O6N27N1auXIknT54oXae8muzn206fPo0h\nQ4bA0dERHh4e+Oqrr1BaWqp0jfJq4Mva8vLysHDhQnTp0gWOjo4YMWIE4uPjVV5PdnY25syZA3d3\nd7i4uGDs2LG4du0axowZAy8vr0q9JyKRCCKRqNwPFOUl4RURi8UYNGgQHj16hFOnTqkcz8/Px4kT\nJ2BrawsnJ6cqvd8VKa8GXiaTYd26dfDy8oKjoyP8/f1x6NChcs9PTk7GokWL0LdvX7i4uMDZ2RmD\nBw/Gvn37lPrNnj0b3333HQCgV69ekEqlSn/+FdXAP3z4EF988QV69OgBBwcH9OjRA1988QWys7OV\n+pWdf/78eWzatAne3t5wcHCAr68vwsLCKvVeVEViYiI+/PBDuLu7w9HREe+++y42bNiAp0+fKvW7\nd+8e5syZA09PTzg4OKBLly4YMWKEUkwymQw///wz+vXrBxcXF7i6usLX1xf//e9/UVJSUu2xE9Hr\n4Qw8EdVraWlpGDduHPz8/ODj44PHjx8DANLT0xESEgIfHx/4+/tDXV0d0dHR2LhxIxISErBp06ZK\nXf/s2bPYuXMnRowYgSFDhiAyMhKbN29G06ZNMWXKlEpdY8KECTAyMsKHH36IR48e4aeffsKkSZMQ\nGRmp+LaguLgY77//PhISEjB48GA4OjoiKSkJ77//Ppo2bVrp90NLSwsDBw7E/v37cfjwYfj7+1f6\n3BcNHjwYa9euRWhoKPz8/JSOHTlyBIWFhRgyZAiA6nu/X7R06VJs3boVb731FsaPH4+srCwsXrwY\nLVq0UOkbHR2NmJgY9OzZE1ZWVopvI+bNm4eHDx9i8uTJAIDhw4cjPz8fJ0+exJw5c2BoaAjg5c9e\n5OXlYeTIkbh9+zaGDBmCDh06ICEhAbt27cKFCxewb98+lW9+goODUVhYiOHDh0MikWDXrl2YPXs2\nrK2tVUrBXtdff/2FMWPGQF1dHaNHj4aJiQlOnz6NlStXIjExUfEtTGlpKd5//32kp6dj1KhRaNWq\nFfLz85GUlISYmBgMGjQIALB27VqsWbMGnp6eGDFiBNTU1JCSkoJTp06huLi4znzTRNToyYmI6rj9\n+/fLbW1t5fv371dq9/T0lNva2sr37t2rck5RUZG8uLhYpT04OFhua2srj4+PV7TdvXtXbmtrK1+z\nZo1Km7Ozs/zu3buKdplMJu/bt6+8W7duStedNWuW3NbWtty2hQsXKrVHRETIbW1t5bt27VK0bd++\nXW5rayv/4YcflPqWtXt6eqq8lvLk5eXJJ06cKHdwcJB36NBBfuTIkUqdV5GxY8fK27dvL09PT1dq\nHzZsmNze3l6elZUll8vf/P2Wy+VyW1tb+axZsxQ/Jycny6VSqXzs2LHy0tJSRfuVK1fkUqlUbmtr\nq/RnU1BQoHL/p0+fyt977z25q6urUnxr1qxROb9M2Xi7cOGCou2bb76R29rayrdv367Ut+zPJzg4\nWOX8AQMGyIuKihTt9+/fl9vb28s/+eQTlXu+qOw9+uKLL17ab/jw4fL27dvLExISFG0ymUw+ffp0\nua2trfyPP/6Qy+VyeUJCgtzW1la+fv36l15v4MCB8j59+rwyPiISFktoiKheMzAwwODBg1XaJRKJ\nYrawtLQUOTk5ePjwIbp27QoA5ZawlKdXr15Kq9yIRCK4u7sjMzMTBQUFlbrG+PHjlX5+++23AQC3\nb99WtJ0+fRpqamoYO3asUt+hQ4dCT0+vUveRyWSYMWMGEhMTcfToUXTv3h2ffvopwsPDlfrNnz8f\n9vb2laqJDwgIwNOnT3HgwAFFW3JyMv788094eXkpHiKurvf7eZGRkZDL5Xj//feVatLt7e3RrVs3\nlf46OjqK/y4qKkJ2djYePXqEbt26IT8/Hzdv3qxyDGVOnjwJIyMjDB8+XKl9+PDhMDIywi+//KJy\nzqhRo5TKlszNzdG6dWv8888/rx3H87KysnDp0iV4eXnBzs5O0S4SiTB16lRF3AAUYygqKgpZWVkV\nXlNXVxfp6emIiYmplhiJqGawhIaI6rUWLVpU+MDhjh07sHv3bty4cQMymUzpWE5OTqWv/yIDAwMA\nwKNHj9CkSZMqX6OsZOPRo0eKtpSUFJiZmalcTyKRwMrKCrm5ua+8T2RkJM6dO4cVK1bAysoKq1ev\nxrRp0/D555+jtLRUUSaRlJQER0fHStXE+/j4QF9fH6GhoZg0aRIAYP/+/QCgKJ8pUx3v9/Pu3r0L\nAGjTpo3KsbZt2+LcuXNKbQUFBfjuu+9w9OhR3Lt3T+WcyryHFUlJSYGDgwPU1ZX/2lRXV0erVq1w\n7do1lXMqGjupqamvHceLMQGAjY2NyrE2bdpALBYr3kNLS0tMmTIF69evh4eHB9q3b4+3334bfn5+\ncHJyUpw3c+ZMfPjhhxg9ejTMzMzg5uaGnj17wtfXt0rPUBBRzWICT0T1mra2drntP/30E5YtWwYP\nDw+MHTsWZmZm0NDQQHp6OmbPng25XF6p679sNZI3vUZlz6+ssocu33rrLQDPkv/vvvsOU6dOxZw5\nc1BaWgo7OzvEx8fjyy+/rNQ1NTU14e/vj507dyIuLg7Ozs44dOgQLCws8M477yj6Vdf7/Sb+85//\n4MyZMxg2bBjeeustGBgYQE1NDWfPnsXPP/+s8qGiptXWkpiV9cknnyAgIABnzpxBTEwMQkJCsGnT\nJgQFBeGzzz4DALi4uODkyZM4d+4coqKiEBUVhcOHD2Pt2rXYuXOn4sMrEQmLCTwRNUgHDx6EpaUl\nNmzYoJRI/frrrwJGVTFLS0ucP38eBQUFSrPwJSUlSElJqdRmQ2WvMzU1Fc2aNQPwLIn/4YcfMGXK\nFMyfPx+WlpawtbXFwIEDKx1bQEAAdu7cidDQUOTk5CAzMxNTpkxRel9r4v0um8G+efMmrK2tlY4l\nJycr/Zybm4szZ85gwIABWLx4sdKxP/74Q+XaIpGoyrHcunULpaWlSrPwpaWl+Oeff8qdba9pZaVd\nN27cUDl28+ZNyGQylbhatGiBMWPGYMyYMSgqKsKECROwceNGBAYGwtjYGADQpEkT+Pr6wtfXF8Cz\nb1YWL16MkJAQBAUF1fCrIqLKqFvTA0RE1UQsFkMkEinN/JaWlmLDhg0CRlUxLy8vPH36FFu3blVq\n37t3L/Ly8ip1jR49egB4tvrJ8/Xtmpqa+Oabb6Cvr4+UlBT4+vqqlIK8jL29Pdq3b4+IiAjs2LED\nIpFIZe33mni/vby8IBKJ8NNPPyktiXj16lWVpLzsQ8OLM/0ZGRkqy0gC/9bLV7a0x9vbGw8fPlS5\n1t69e/Hw4UN4e3tX6jrVydjYGC4uLjh9+jSuX7+uaJfL5Vi/fj0AoHfv3gCeraLz4jKQmpqaivKk\nsvfh4cOHKvext7dX6kNEwuMMPBE1SH5+fvj6668xceJE9O7dG/n5+Th8+HCVEtfaNHToUOzevRur\nVq3CnTt3FMtIHjt2DC1btlRZd7483bp1Q0BAAEJCQtC3b18MGDAAFhYWuHv3Lg4ePAjgWTL2/fff\no23btujTp0+l4wsICMCSJUvw22+/wc3NTWVmtybe77Zt22L06NHYvn07xo0bBx8fH2RlZWHHjh2w\ns7NTqjvX1dVFt27dcOjQIWhpacHR0RGpqanYs2cPrKyslJ43AABnZ2cAwMqVK9GvXz9oamqiXbt2\nsLW1LTeWoKAgHDt2DIsXL8a1a9fQvn17JCQkICQkBK1bt66xmekrV67ghx9+UGlXV1fHpEmTMHfu\nXIwZMwajR4/GqFGjYGpqitOnT+PcuXPw9/dHly5dADwrr5o/fz58fHzQunVrNGnSBFeuXEFISAic\nnZ0Vify7776Ljh07wsnJCWZmZsjMzMTevXuhoaGBvn371shrJKKqq5t/kxERvaEJEyZALpcjJCQE\nX375JUxNTdGnTx8MGTIE7777rtDhqZBIJNiyZQuWL1+OyMhIHD16FE5OTvj5558xd+5cFBYWVuo6\nX375Jdzc3LB7925s2rQJJSUlsLS0hJ+fHwIDAyGRSDB8+HB89tln0NPTg4eHR6Wu269fPyxfvhxF\nRUUqD68CNfd+z507FyYmJti7dy+WL1+OVq1aYcGCBbh9+7bKg6MrVqzA119/jVOnTiEsLAytWrXC\nJ598AnV1dcyZM0epb6dOnfDpp59i9+7dmD9/PkpLSzFt2rQKE3g9PT3s2rULa9aswalTpxAaGgpj\nY2OMGDECH330UZV3/62s+Pj4clfwkUgkmDRpEhwdHbF7926sWbMGu3btwuPHj9GiRQt8+umnCAwM\nVPSXSqXo3bs3oqOjER4eDplMhmbNmmHy5MlK/QIDA3H27Fls27YNeXl5MDY2hrOzMyZPnqy00g0R\nCUskr40ni4iI6LU8ffoUb7/9NpycnF57MyQiImpYWANPRFRHlDfLvnv3buTm5pa77jkRETVOLKEh\nIqoj5s2bh+LiYri4uEAikeDSpUs4fPgwWrZsiWHDhgkdHhER1REsoSEiqiMOHDiAHTt24J9//sHj\nx49hbGyMHj16YMaMGTAxMRE6PCIiqiOYwBMRERER1SOsgSciIiIiqkeYwBMRERER1SN8iLWKsrML\nIJPVftWRsbEusrLya/2+1DhwfFFN4viimsTxRQ2RWCyCoWGTCo8zga8imUwuSAJfdm+imsLxRTWJ\n44tqEscXNTYsoSEiIiIiqkeYwBMRERER1SNM4ImIiIiI6hEm8ERERERE9QgTeCIiIiKieoSr0BAR\nERFVgydPCpCfn4OnT0uEDoXqMDU1DejqNoW2dsXLRL4KE3giIiKiN1RSUoy8vGwYGJhAQ0MTIpFI\n6JCoDpLL5SgpKcKjRw+grq4BDQ3Ja12HJTREREREbygv7xF0dZtCItFi8k4VEolEkEi00KRJU+Tn\nP3rt6zCBJyIiInpDpaXF0NTUFjoMqie0tLRRUlL82uezhKaOO3/1PkLPJuNhbhGM9DUxuEdbdLG3\nEDosIiIieo5M9hRisZrQYVA9IRarQSZ7+trnC5rAFxcXY/Xq1Th48CByc3NhZ2eHTz75BF26dHnp\neSdOnEBERAQuX76MrKwsNGvWDJ6envjggw+gp6en1Dc9PR0rVqzAb7/9hsLCQkilUkyfPh0eHh41\n+dKqxfmr97HlaCKKS2UAgKzcImw5mggATOKJiIjqGJbOUGW96VgRtIRm9uzZ2LJlC/r374+5c+dC\nLBZj4sSJuHTp0kvPmz9/PpKTkzFgwADMmzcPHh4e2LZtG0aOHImioiJFv9zcXIwcORKnT5/GqFGj\n8Pnnn0MikWDSpEk4f/58Tb+8NxZ6NlmRvJcpLpUh9GyyQBERERERkdAEm4G/fPkyjhw5gjlz5mD8\n+PEAgIEDB8Lf3x8rV67Ejh07Kjx3zZo1cHd3V2pzcHDArFmzcOTIEQwePBgAsHv3bqSmpmL79u14\n6623AAAjR47EsGHDsGzZMhw8eLBmXlw1ycotqlI7ERERUX0zbdokAMB3362v1XPrM8ES+GPHjkFD\nQwNDhw5VtGlqaiIgIADBwcHIyMiAmZlZuee+mLwDgLe3NwAgOfnf2em4uDiYmpoqkncAEIvF6NOn\nD5YvX46bN2+iTZs21fWSqp2xvma5ybqOpjpkMjnEYn5VR0RERDXDw6Nzpfrt23cIzZo1r+Fo6HmC\nJfAJCQlo3bo1mjRRXsTeyckJcrkcCQkJFSbw5Xnw4AEAwNDQUNFWUlICLS0tlb5lbdeuXavTCfzg\nHm2VauABQCQCHheVYtnOOAT1bQ8zQx0BIyQiIqKGav78xUo/7927C+np9/DRRzOV2g0MDPEmgoO/\nF+Tc+kywBD4zMxPm5uYq7aampgCAjIyMKl1vw4YNUFNTg4+Pj6KtdevWOH/+PO7fvw8Li38f+oyN\njX2te9S2sgdVn1+FZlD3NhBBhO0nr2Ph5osY3ssGPZyb88EZIiIiqla+vu8q/XzmTCRych6ptL+o\nsLCw3AnUimhoaLxWfG96bn0mWAJfWFhY7puuqakJAEoPo75KeHg4QkJCMHnyZFhbWyvaAwICsHv3\nbsyYMQOzZ8+GiYkJIiIicPLkSUUMVWVsrFvlc95E/5566N+znUp7l45WWLPnErYeS8LVf7IxfbgL\njPQr/z8L0YtMTfVe3YnoNXF8UU2qC+MrI0MMdfWGvb1O2WTh869z6tSJyM/Pw+zZ87B69TdISkrA\ne++Nw8SJU/Drr2dw4EAorl9PRE5ODszMzNG3bz+MGxcINTU1pWsAwNq1GwAAsbEx+PDDSVi6dAVu\n3bqJsLAQ5OTkwMnJGbNmzUWLFtbVci4AhITswc6d25GV9QBt29pg+vRPsG7dWqVr1hSxWPzaY1ew\nBF5LSwslJSUq7WWJe1ki/yoxMTGYO3cuevbsiRkzZigds7Ozw8qVK7Fw4UKMGDECwLMZ/v/+979Y\ntGgRdHSqXn6SlZUPmUxe5fPelKmpHjIz85Tapg12wOm4VOw7fQMffBWJMb5SuLVX/VaD6FXKG19E\n1YXji2pSXRlfMpkMpS+sHPemyvaCycotgnEd2AtGLn+W/zz/OuVyObKzs/Gf/8yAj48ffH3fhbm5\nBUpLZQgPPwQtLW0MGzYaOjraiI2Nwfr1a5GXl48PP5xR4XWfPn32759+2gixWA0jR45FXl4udu3a\nhgUL5mLDhi3Vcm5YWAi+/vordOzoimHDRuLevXv4/PP/QE9PD6amZtX+5/kimUxW4dgVi0UvnTQW\nLIE3NTUtt4QlMzMTACpV/56YmIipU6dCKpUiODhY6dNcGT8/P3h5eSExMREymQwdOnRAdHQ0AKBV\nq1Zv9iIEJhaJ0KuTFexbG2Hj4Wv48eBVXPr7AUb3toWuduP8SomIiKghqE97wTx4kInZs+fD33+A\nUvuiRf8HTc1/qwMGDgzAihX/Q1jYPkycOBUSieSl1y0tLcXmzVugrv4sXdXXb4rVq1fi5s0baNPG\n5o3OLSkpwcaNa2Fv74hVq35Q9LOxaYcvv1wEU9PKP4cpBMESeDs7O2zbtg0FBQVKD7LGx8crjr/M\nnTt3EBQUBCMjI6xbt+6ls+kSiQROTk6Kn//44w9IJBK4urq+4auoGyyMdDDnPVdEXLiDQ+duIelO\nNgLfbQ+HNsZCh0ZERNSo/f7XPZy7fK/K5yWn5aD0qfI3/sWlMvwUkYBf/0yr8vU8nJqhm2OzKp9X\nGVpaWvDz66vS/nzy/vhxAYqLS+Ds7IKDB0Nx+/Y/aNfO9qXX7du3vyKxBgBn544AgLS01Fcm8K86\nNzHxGnJycvDBB4OU+vXu7Yc1a7556bXrAsESeD8/P2zevBn79u1TrANfXFyM0NBQuLq6Kh5wTUtL\nw5MnT9C2bVvFuZmZmQgMDIRIJMKmTZtgZGRU6fv+888/2L17NwYNGgR9ff1qfU1CUhOL0a9rKzi1\nMcaGw9fwzd549HSxxHBPG2hKuLUzERFRffJi8v6qdiGZmpopJcFlbt5MxoYNaxEXdxEFBQVKxwoK\n8l95XXNz5W8a9PSe5W15ea8umXrVuffvP/tQZWXVQqmfuro6mjWrmQ861UmwBN7Z2Rl+fn5YuXIl\nMjMzYW1tjbCwMKSlpWHp0qWKfrNmzUJ0dDSSkpIUbUFBQbh79y6CgoIQGxurWFUGAKytreHi4gLg\n2dcnAwYMgK+vL5o1a4aUlBTs3r0bzZs3x6efflp7L7YWtbTQw8LxnRH6602ciL6La7ceIsi/A2ys\nmgodGhERUaPTzfH1Zr4/++H3cveCMdbXxKzRdauC4PmZ9jJ5eXn46KNJ0NHRxYQJU2BpaQWJRILr\n1xOxdu23kMleXV8uFpc/AVlW915T59YHgiXwALB8+XKsWrUKBw8eRE5ODqRSKdavX49OnTq99LzE\nxGc1YBs3blQ5NmjQIEUCLxaL0a5dO+zfvx9ZWVkwMTHBwIEDMW3aNOjpCf/Eek3RUFfDcK926Ghj\ngk1HErB0Ryzefbsl+ndrDY0G/oQ8ERFRQ1DeXjASdTEG92j7krPqjkuXYpGTk4Mvv1yBjh3//cBx\n717Vy39qgoXFsw9VKSl34ezsomgvLS3FvXv30Lbty0t0hCZoAq+pqYlZs2Zh1qxZFfbZtm2bStvz\ns/EvIxaLsWrVqteOr76TWhvii0A37Dn1N46cv434G1mY2K8DWpjV7lKYREREVDXP7wVTV1ahqQqx\n+NmE4fMz3iUlJQgL2ydUSErs7DqgadOmOHQoDL6+7ypKgE6ePIa8vFyBo3s1QRN4qnnamuoY36c9\nOrYzxc9HE7Fky0UMeqcNfN2sIRZz8yciIqK6qou9Rb1J2F/k6OgEPT19fPnlIgQEDIdIJMLx4xGo\nKxUsGhoaCAychODgFfj44w/g6dkL9+7dw9Gj4bC0tKrzG2SynqKR6GhjgiUT3OBsY4J9Z5KxbGcc\nMrIfCx0WERERNUBNmxpg+fJgGBubYMOGtdi1azs6d3bHBx9MFzo0hSFDhuPjjz/F/fv38P33qxEf\nfwnLln0DXV09SCSV249IKCJ5Q6nmryV1aSOn1yGXy3Hhajq2n7wOmUyO4b1s0MO5eZ3/pEk1q65s\nhEINE8cX1aS6Mr7u378NC4uWQodBb0gmk8Hfvzd69PDErFnzavReLxszr9rIiTPwjYxIJEIXBwss\nmeCGtpb62HosCav2XUZ2nuqT7kREREQNVVGRau5z7NgR5ObmwMXl5QuqCI018I2Ukb4WZg7viNNx\nqdh3+gYWbIrCGF8p3NqbCx0aERERUY27fPlPrF37LXr29IK+flNcv56II0cOoU2btvD09BY6vJdi\nAt+IiUUi9OpkBfvWRth4+Bp+PHgVl/5+gNG9baGrrSF0eEREREQ1pnlzS5iYmCIkZA9yc3Ogr98U\nfn59MWXKNGho1O08iAk8wcJIB3Pec0XEhTs4dO4Wku5kI/Dd9nBoYyx0aEREREQ1wtLSCsuXBwsd\nxmthDTwBANTEYvTr2grzxnZGEy0NfLM3HluPJ6Go+KnQoRERERHRc5jAk5KWFnpYML4z/NyscfZS\nKhZujsaNlByhwyIiIiKi/48JPKnQUFfDMC8bfD7KBTK5HEt3xCLkTDJKntvOmYiIiIiEwQSeKiS1\nNsQXgW54x6kZIi7cxpItMbibkS90WERERESNGhN4eiltTXWM79Me0wOckPu4GEu2XETEhduCbGZF\nREREREzgqZI62phgyQQ3ONuYIORMMpbtjENG9mOhwyIiIiJqdJjAU6Xp6UjwwUAHTOzXAamZBVi4\n+SLO/JkKuZyz8URERES1hQk8VYlIJEIXewssmeCGtpb62HosCav2XUZ2nup2xERERERlIiLC4eHR\nGffupSnaAgL64csvF73WuW8qLi4GHh6dERcXU23XrC1M4Om1GOlrYebwjhjd2xZJd7KxYFMUohPS\nhQ6LiIiIqsnnn38Cb28PPHnypMI+M2dOg69vDxQV1d2JvF9+OY69e3cKHUa1YgJPr00sEqFXJyss\nCnSDuZEOfjx4FT8evIL8JyVCh0ZERERvqHdvXxQWFuLcubPlHs/OfojY2Ivo3t0Tmpqar3WPnTv3\nY9aseW8S5itFRp7A3r27VNo7dnRFZOTv6NjRtUbvXxOYwNMbszDSwZz3XDGoexvEJmVi/qYo/HUz\nS+iwiIiI6A28805PaGvr4Jdfjpd7/NSpX/D06VP4+Pi99j0kEgnU1dVf+/w3IRaLoampCbG4/qXD\nwrxj1OCoicXo17UVnNoYY+PhawjeG4+eLpYY7mkDTYma0OERERFRFWlpaeGdd3rg9OlfkJubC319\nfaXjv/xyHMbGxmjRoiVWrlyG2NhopKenQ0tLC66unfHhhzPQrFnzl94jIKAfXFw6Ye7cRYq2mzeT\nsWrVCly58heaNm2KAQMGw8TEVOXc3347g0OHwnD9ehJyc3NgamqGd9/thzFj3oea2rPcY9q0Sfjz\nzzgAgIdHZwCAhUUzhISEIy4uBtOnT8GaNT/C1bWz4rqRkSewffvPuH37H+joNEG3bu9g6tTpMDAw\nUPSZNm0S8vPzsWDBYnzzzT2hTkgAACAASURBVHIkJFyFnp4+hg4dgdGjx1XtjX4NTOCpWrW00MOC\n8Z0R9ustHI++g2u3HiLIvwNsrJoKHRoREVG9En0/DoeSjyG76BEMNQ3Qv60f3Cxqt9yjd28/nDhx\nFGfORKJ//0GK9vv37+HKlcsICBiBhISruHLlMry9fWFqaoZ799Jw4MB+fPTRZGzfvg9aWlqVvl9W\n1gNMnz4FMpkM7703Dlpa2jh0KKzcEp2IiMPQ1tbB8OGjoaOjjdjYGGzc+CMKCgrw4YczAADjxgXi\nyZMnSE+/h48+mgkA0NbWqfD+ERHh+N//voC9vSOmTp2OjIx07N+/BwkJV7Fhw1alOHJzc/Cf/0yH\np2cv9Orlg9Onf8Hatd+iTRsbdOnSrdKv+XUwgadqp6GuhmFeNnC2McamIwlYuiMWfdxbYoBHa2io\n17+vqYiIiGpb9P047EzcjxLZs+fKsoseYWfifgCo1ST+rbfcYWBgiF9+Oa6UwP/yy3HI5XL07u2L\ntm1t4OnprXRet27dMWXK+zhzJhJ+fn0rfb8dO7YgJ+cRNm7cBqnUDgDQp48/Ro4cpNJ30aL/g6bm\nvx8OBg4MwIoV/0NY2D5MnDgVEokEb731NkJD9yEn5xF8fd996b1LS0uxdu23sLGxxbffroNEIgEA\nSKV2WLRoLsLDwxAQMELRPyMjHQsX/h96935WQuTvPwABAf44cuQgE3iqv6TWhvgi0A17Tv2NiAu3\ncTk5CxP7dUALM12hQyMiIqoVUfdicf7exSqfdyvnDkrlpUptJbIS7EgIwR9p0VW+Xpdmb8G9Wacq\nn6eurg4vL28cOLAfDx48gImJCQDgl19OwMqqBTp0cFDqX1paioKCfFhZtYCurh6uX0+sUgJ//vzv\ncHR0ViTvAGBoaIjevfsgLGyfUt/nk/fHjwtQXFwCZ2cXHDwYitu3/0G7drZVeq2JideQnf1QkfyX\n8fLqje+/X40//vhdKYHX1dWFt7ev4mcNDQ20b2+PtLTUKt33dTCBpxqlramO8X3ao2M7U/x8NBGL\nf76IQd3bwM/NGmKxSOjwiIiI6qQXk/dXtdek3r39EBq6D6dOncCwYaPwzz+3cOPGdbz//kQAQFFR\nIbZt+xkREeHIzMxQ2uAxPz+/SvdKT78PR0dnlXZr65YqbTdvJmPDhrWIi7uIgoICpWMFBVW7L/Cs\nLKi8e4nFYlhZtUB6+j2ldjMzc4hEyrmMnp4+kpNvVPneVcUEnmpFRxsTLJnghq3HkxByJhl/3niA\noL7tYWZYcR0aERFRfeferNNrzXzP+/1/yC56pNJuqGmAj12nVEdolebo6IxmzSxx8uQxDBs2CidP\nHgMARelIcPAKRESEY+jQkXBwcISuri4AERYt+m+N7dael5eHjz6aBB0dXUyYMAWWllaQSCS4fj0R\na9d+C5lMViP3fZ5YXP4iHbWxQz0TeKo1ejoSfDDQAReupWP7ietYuPkihnvZoEfH5iqfYImIiBqz\n/m39lGrgAUBDrIH+bV9/ycY34e3tg23bfkJKyl1ERp6AVNpeMVNdVuf+0UefKPoXFRVVefYdAMzN\nLZCSclel/c6d20o/X7oUi5ycHHz55QqlddzL36m1cjmGhUUzxb2ev6ZcLkdKyl20bt22UtepDYI+\nUVhcXIwVK1bAw8MDTk5OGDZsGM6fP//K806cOIGPP/4YXl5ecHZ2hp+fH7766ivk5eWp9M3Ly8NX\nX30FHx8fODk5wcvLCwsWLEB6OncNFYJIJEIXewssmeCGtpb62Ho8CcH74pGdV3d3cCMiIqptbhau\nGGU3BIaaz5YuNNQ0wCi7IbW+Ck0ZH58+AIDvvgtGSspdpbXfy5uJ3r9/D54+fVrl+3Tp0g1//RWP\npKRERVt2djZOnjyq1K9s7fbnZ7tLSkpU6uQBQFtbu1IfJuzsOsDQ0AgHDoSgpOTfD06nT0ciMzMD\nXbvW7IOpVSHoDPzs2bNx4sQJjB07Fi1btkRYWBgmTpyIbdu2wcXFpcLz5s+fDzMzMwwYMADNmzdH\nUlIStm3bht9++w379+9XLPEjk8kwYcIE/P333xg5ciRat26NW7duYdeuXbhw4QIOHz6s9JAC1R4j\nfS3MHN4Rp+NSse/0DSzYFIUxvlK4tTcXOjQiIqI6wc3CVbCE/UWtW7eBjY0tzp37FWKxGL16/fvw\nZteuHjh+PAJNmuiiVavWuHr1L8TERKNp06ovIT1q1DgcPx6BmTM/REDACGhqauHQoTCYmzdDfv7f\nin6Ojk7Q09PHl18uQkDAcIhEIhw/HoHyqlekUjucOHEU3377DezsOkBbWwceHt1V+qmrq2Pq1I/w\nv/99gY8+mgxvbx9kZKQjJGQP2rRpi379VFfCEYpgCfzly5dx5MgRzJkzB+PHjwcADBw4EP7+/li5\nciV27NhR4blr1qyBu7u7UpuDgwNmzZqFI0eOYPDgwQCAv/76C/Hx8ViwYAFGjx6t6Nu8eXMsWbIE\ncXFxePvtt6v/xVGliEUi9OpkBfvWRth4+Bp+PHgVcdcz8Z6PFLraGkKHR0RERM/x8fHDjRvX4eLS\nSbEaDQDMmPEpxGIxTp48iqKiYjg6OmPVqu8xc+ZHVb6HiYkJ1qxZh+Dg5di27WeljZyWLVui6Ne0\nqQGWLw/Gd9+twoYNa6Gnpw8fnz7o3NkNM2dOU7rmgAFDcP16IiIiDmPPnp2wsGhWbgIPAO++2w8S\niQQ7dmzB99+vRpMmTdC7tx+mTPmo3LXohSKS10alfTmWL1+OrVu3IioqCk2aNFG0r1u3DsHBwfj1\n119hZmZW6evl5+ejU6dOCAoKwmeffQYA+P333xEYGIjVq1fDz+/fr3qOHTuGGTNmYM+ePejYsWOV\n4s7KyodMVvtvmampHjIzVUuEGoqnMhkiLtzBoXO3oKujgcB328OxjbHQYTUaDX18kbA4vqgm1ZXx\ndf/+bVhYqK6UQlSRl40ZsVgEY+OKl90WrAY+ISEBrVu3VkreAcDJyQlyuRwJCQlVut6DBw8APFsr\ntIy9vT10dHSwevVqnD9/Hunp6Th//jxWr14Nd3d3ODurLlNEwlATi9GvayvMG9sZuloaCN4bj63H\nk1BYXPvLZRERERHVZYIl8JmZmeXOsJuamgIAMjIyqnS9DRs2QE1NDT4+Poo2AwMDBAcHIy8vD+PH\nj0f37t0xfvx4tGzZEuvXr+fKJ3VQSws9LBjfGX5u1jh7KRWLNl/EjZQcocMiIiIiqjMEq4EvLCyE\nhoZqnXNZfVFRUeVXJQkPD0dISAgmT54Ma2trpWNGRkZwcHCAi4sL2rZti8TERGzcuBH//e9/8c03\n31Q57pd9nVHTTE31BLt3bftwuAt6dG6B4N2XsGxHLAZ7tsMoXyk01Mtfc5XeXGMaX1T7OL6oJtWF\n8ZWRIYa6uqCL+1E9IxaLX3vsCpbAa2lpKS3RU6Ysca/sgwIxMTGYO3cuevbsiRkzZigdu3v3LsaO\nHYuVK1fC29sbAODt7Q1LS0vMnj0bQ4YMQbduVVsSiDXwtcdcXxMLx3XGnlN/I+TU37jw1z1M7NcB\nLcyE+xDVUDXG8UW1h+OLalJdGV8ymQylpTW/eRA1HDKZrMKxW2dr4E1NTcstk8nMzASASj3AmpiY\niKlTp0IqlSI4OBhqasqzs6GhoSguLkaPHj2U2r28vAAAcXFxrxs+1RJtTXWM79Me0wOckPu4GIt/\nvoiIC7cF+RBFREREVBcIlsDb2dnh1q1bKCgoUGqPj49XHH+ZO3fuICgoCEZGRli3bh10dHRU+mRl\nZUEul6tsaVtaWqr0b6r7OtqYYMkEN3RsZ4KQM8lYtjMOGdmPhQ6LiIiIqNYJlsD7+fmhpKQE+/b9\nu2NWcXExQkND4erqCnPzZxv6pKWlITk5WenczMxMBAYGQiQSYdOmTTAyMir3Hq1atYJMJsPRo8q7\ndx0+fBgA0KFDh+p8SVTD9HQk+GCgAyb264DUzAIs3HwRZy6lqnxAIyIiImrIBKuBd3Z2hp+fH1au\nXInMzExYW1sjLCwMaWlpWLp0qaLfrFmzEB0djaSkJEVbUFAQ7t69i6CgIMTGxiI2NlZxzNraWrGL\n66BBg7B582bMnTsXV65cgY2NDa5evYqQkBBIpVJFKQ3VHyKRCF3sLSBtYYDNEQnYejwJcX9n4v0+\n7WGoV3c2WCAiosZHLpdzhTuqlDedfBRsIyfg2QOrq1atQnh4OHJyciCVSjFz5kx07dpV0WfMmDEq\nCbxUKq3wmoMGDcKyZcsUP6enp2P16tWIiopCeno6DAwM4OXlhU8++URpzfjK4kOsdYdMLsfpuFTs\nO30DGupijPGVwq29udBh1UscX1STOL6oJtWV8ZWZmYqmTU0gkXAyiV6tuLgIOTkPYGpqWe7xVz3E\nKmgCXx8xga977j98jI2Hr+FmWi7c2pvhPR8pdLVVlyilinF8UU3i+KKaVFfG15MnBcjLy4aBgSk0\nNCSciadyyeVylJQU49GjTOjpGUJbu0m5/V6VwAtWQkNUXSyMdDDnPVdEXLiDQ+duIenuIwS+2x6O\nbYyFDo2IiBqJskQsJ+cBnj7lIhlUMTU19Zcm75XBBJ4aBDWxGP26toJTG2NsPHwNwXvj0dPFEsM8\n20JLwmFOREQ1T1u7yRslZUSVxS3DqEFpaaGHBeM7w8/NGmcvpWLR5ou4kZIjdFhERERE1YYJPDU4\nGupqGOZlg89HuUAml2PpjliEnElGCXfIIyIiogaACTw1WFJrQ3wR6IZ3nJoj4sJtLNkSg7sZ+UKH\nRURERPRGmMBTg6atqY7xfewwPcAJuY+Lsfjni4i4cFuQlYSIiIiIqgMTeGoUOtqYYMkEN7i0M0HI\nmWQs2xmHjOzHQodFREREVGVM4KnR0NORYOpAB0zq1wFpmQVYuPkizlxKfePd0IiIiIhqExN4alRE\nIhHetrfA4glusLHUx9bjSQjeF4/svCKhQyMiIiKqFCbw1CgZ6Wth5vCOeM/HFtfvPMKCTVGITkgX\nOiwiIiKiV2ICT42WSCSCl6sVFgW6wdxIBz8evIofD15B/pMSoUMjIiIiqhATeGr0LIx0MOc9Vwzq\n3gaxSZmYvykKf93MEjosIiIionIxgScCoCYWo1/XVpg3tjN0tTQQvDceW48nobC4VOjQiIiIiJQw\ngSd6TksLPSwY3xl+btY4eykVizZfxN8pj4QOi4iIiEiBCTzRCzTU1TDMywafj3KBTC7Hsh1xCDmT\njJJSmdChERERETGBJ6qI1NoQXwS64R2n5oi4cBtLtsTgbka+0GERERFRI8cEnugltDXVMb6PHWYE\nOCH3cTEW/3wRERduQybj5k9EREQkDCbwRJXgbGOCJRPc4NLOBCFnkrFsRxwysh8LHRYRERE1Qkzg\niSpJT0eCqQMdMKlfB6Q9KMDCzRdx5lIq5HLOxhMREVHtYQJPVAUikQhv21tg8QQ32FjqY+vxJATv\ni0d2XpHQoREREVEjwQSe6DUY6Wth5vCOeM/HFtfvPMKCTVGITkgXOiwiIiJqBJjAE70mkUgEL1cr\nLAp0g4WRDn48eBU/HryC/CclQodGREREDRgTeKI3ZGGkg9nvuWJw9zaITcrE/E1R+OtmltBhERER\nUQPFBJ6oGqiJxfDv2grzxnaGrpYGgvfGY+vxJBQWlwodGhERETUwTOCJqlFLCz0sGN8Zfu7WOHsp\nFYs2X8TfKY+EDouIiIgaEEET+OLiYqxYsQIeHh5wcnLCsGHDcP78+Veed+LECXz88cfw8vKCs7Mz\n/Pz88NVXXyEvL0+pX2hoKKRSaYX/HDp0qKZeGjViGupqGOZpg1mjXSGTy7FsRxxCziSjpFQmdGhE\nRETUAIjkAi5iPXPmTJw4cQJjx45Fy5YtERYWhitXrmDbtm1wcXGp8Dx3d3eYmZnB29sbzZs3R1JS\nEnbv3o1WrVph//790NTUBADcvXsXcXFxKudv2bIFiYmJOHv2LExNTasUc1ZWviC7cJqa6iEzM+/V\nHalOeVJUij2nbuDX+DRYmepiYr8OaGGmK3RYKji+qCZxfFFN4viihkgsFsHYuOJ8QbAE/vLlyxg6\ndCjmzJmD8ePHAwCKiorg7+8PMzMz7Nixo8Jzo6Ki4O7urtR24MABzJo1C0uXLsXgwYMrPLewsBBd\nu3ZFx44dsXnz5irHzQSeXkf8jQf46WgiCp6UYOA7rdHHvSXEYpHQYSlwfFFN4viimsTxRQ3RqxJ4\nwUpojh07Bg0NDQwdOlTRpqmpiYCAAMTGxiIjI6PCc19M3gHA29sbAJCcnPzS+546dQoFBQXo16/f\na0ZOVHXONiZYMsENLu1MsP/sTSzbEYeM7MdCh0VERET1kGAJfEJCAlq3bo0mTZootTs5OUEulyMh\nIaFK13vw4AEAwNDQ8KX9wsPDoaWlhd69e1ctYKI3pKcjwdSBDpjUrwPSHhRg4eaLOH0pFQJWsRER\nEVE9JFgCn5mZCTMzM5X2spr0l83Al2fDhg1QU1ODj49PhX0ePXqE3377DZ6entDVrXt1yNTwiUQi\nvG1vgcUT3GBjqY9tx5MQvC8e2XlFQodGRERE9YS6UDcuLCyEhoaGSnvZA6hFRZVPaMLDwxESEoLJ\nkyfD2tq6wn7Hjx9HSUnJG5XPvKweqaaZmuoJdm+qXqamelg6zQQRf/yDzeFXsXBzND4Y4ox3XCwF\njYmopnB8UU3i+KLGRrAEXktLCyUlqlvOlyXuZYn8q8TExGDu3Lno2bMnZsyY8dK+4eHhMDAwQPfu\n3ase8P/Hh1ipOrnZmqDl+29h4+FrWL49Bmdi7+A9Hyl0tVU/3NYkji+qSRxfVJM4vqghqrMPsZqa\nmpZbJpOZmQkA5ZbXvCgxMRFTp06FVCpFcHAw1NTUKuyblpaGmJgY+Pr6ljvzTyQUcyMdzH7PFYO7\nt0FsUibmb4rCXzezhA6LiIiI6ijBEng7OzvcunULBQUFSu3x8fGK4y9z584dBAUFwcjICOvWrYOO\njs5L+x8+fBhyuRz9+/d/s8CJaoCaWAz/rq0wb2xn6GprIHhvPLYeS0RhcanQoREREVEdI1gC7+fn\nh5KSEuzbt0/RVlxcjNDQULi6usLc3BzAs5nzF5eGzMzMRGBgIEQiETZt2gQjI6NX3u/w4cNo3rw5\nOnXqVL0vhKgatbTQw4JxneHnbo2zf6Zh4eZo/J3ySOiwiIiIqA4RrAbe2dkZfn5+WLlyJTIzM2Ft\nbY2wsDCkpaVh6dKlin6zZs1CdHQ0kpKSFG1BQUG4e/cugoKCEBsbi9jYWMUxa2trlV1cr1+/jqSk\nJEyaNAkiUd3ZPIeoPBrqahjmaYOONibYePgalu2IQx/3lhjg0Roa6oJ95iYiIqI6QrAEHgCWL1+O\nVatW4eDBg8jJyYFUKsX69etfOUuemJgIANi4caPKsUGDBqkk8OHh4QAAf3//aoqcqObZtjDAF4Fu\n2HPqBiIu3Mbl5CxM7NcBLcy4BCoREVFjJpJzF5kq4So0JIT4Gw/w09FEFDwpwcB3WqOPe0uIxdX3\nbRLHF9Ukji+qSRxf1BDV2VVoiKjynG1MsGSCG1zamWD/2ZtYtiMO6dmPhQ6LiIiIBMAEnqie0NOR\nYOpAB0zq1wFpDwqwcHM0Tl9KBb9EIyIialyYwBPVIyKRCG/bW2DxBDe0s2yKbceTELwvHtl5ld+5\nmIiIiOo3JvBE9ZCRvhZmDu+I93xscf3OIyzYFIWoa+lCh0VERES1gAk8UT0lEong5WqFLwLdYGGk\ng3WHruLHg1eQ/6RE6NCIiIioBjGBJ6rnzI10MPs9Vwzu3gaxSZmYvykKl5OzhA6LiIiIaggTeKIG\nQE0shn/XVpg/rjN0tTWwal88th5LRGFxqdChERERUTVjAk/UgFib62HBuM7wc7fG2T/TsHBzNP5O\neSR0WERERFSNmMATNTAa6moY5mmDWaNdIZcDy3bEYd+ZGygplQkdGhEREVUDJvBEDZRtCwN8EeiG\nd5ya4+iFO1iy5SLupHO3QiIiovqOCTxRA6atqY7xfewwI8AJeY9LsGRLDI6c/wcyGTd/IiIiqq+Y\nwBM1As42JlgS5A4XW1PsP3sTy3bEIT37sdBhERER0WsQybkPe5VkZeULMntpaqqHzEyWP9Cbkcvl\niLqWju0nrqNUJsNbdmZIvJ2Nh7lFMNLXxOAebdHF3kLoMKmB4e8vqkkcX9QQicUiGBvrVnhcvRZj\nISKBiUQivG1vAam1Ib7Z8yd+/+u+4lhWbhG2HE0EACbxREREdRhLaIgaIUM9zXLXiC8ulSH0bLIA\nEREREVFlMYEnaqSycosqbD9/5T6XnSQiIqqjmMATNVLG+prltovFImw4fA2f/vA7Qn9NxsPcwlqO\njIiIiF6GNfBEjdTgHm2x5Wgiip+baZeoizHWT4qmupqIjEnBkT9uI+L8HbjYmqCXqxWk1gYQiUQC\nRk1ERERM4IkaqbIHVUPPJpe7Co19KyM8ePQEpy+l4tf4NMQmZcLStAl6uVrhbXtzaEn464OIiEgI\nXEayiriMJDVErxpfxSVPEZWQjsjYFNxJz4e2pjo8HJvBy9US5kY6tRgp1Uf8/UU1ieOLGiIuI0lE\nb0yioYZ3nJrDw7EZktNyERmbglNxKTgZcxcObYzQy9UKjm2NIWZ5DRERUY1jAk9ElSYSiWBj2RQ2\nlk0x3MsGv/6ZhtN/pmJ1yGWYGmjB08UK7zg3QxMtDaFDJSIiarBYQlNFLKGhhuhNxlfpUxnirmfi\nVGwKrqfkQKIuxtv2FvBytYS1uV41R0r1EX9/UU3i+KKGiCU0RFSj1NXEcGtvDrf25riTnodTcam4\ncPU+fo1Pg61VU3h1soKrrSnU1bhqLRERUXXgDHwVcQaeGqLqHl8FhSU4d/keTsWlIPNRIQx0JejZ\n0RI9OjZHU93y15+nhou/v6gmcXxRQ/SqGXhBE/ji4mKsXr0aBw8eRG5uLuzs7PDJJ5+gS5cuLz3v\nxIkTiIiIwOXLl5GVlYVmzZrB09MTH3zwAfT0VL+yz8jIwOrVq3H27Fnk5OTA3NwcvXr1wpw5c6oc\nMxN4aohqanzJZHL8dTMLkXEpuHLzIdTEInS2M0MvVyu0tdTnmvKNBH9/UU3i+KKGqE6X0MyePRsn\nTpzA2LFj0bJlS4SFhWHixInYtm0bXFxcKjxv/vz5MDMzw4ABA9C8eXMkJSVh27Zt+O2337B//35o\nav47w5eamoqRI0dCV1cXY8eOhaGhIe7fv49bt27VxkskatTEYhGcbUzgbGOC9IePcSouFef+SkPU\ntXS0NNeDVydLuLc3h0RDTehQiYiI6g3BZuAvX76MoUOHYs6cORg/fjwAoKioCP7+/jAzM8OOHTsq\nPDcqKgru7u5KbQcOHMCsWbOwdOlSDB48WNE+YcIE5OXlYevWrdDS0nrjuDkDTw1RbY6vwuJSXLj6\nbE351AcFaKKlju7OzeHpYgkTA+1aiYFqF39/UU3i+KKGqM7OwB87dgwaGhoYOnSook1TUxMBAQEI\nDg5GRkYGzMzMyj33xeQdALy9vQEAycnJirbk5GScO3cO69evh5aWFp48eQINDQ2oq/PZXSKhaEnU\n0dPlWT180p1HiIxLwfHouzgWdQfONibo1ckKHVoZsryGiIioAoJlsgkJCWjdujWaNGmi1O7k5AS5\nXI6EhIQKE/jyPHjwAABgaGioaPvjjz8AABKJBIMHD8bVq1ehoaEBLy8vLFq0CEZGRtXwSojodYhE\nIti1NIRdS0M8zC3EmT9TcfbPNPx54wEsjHTQq5MVujpYQFuTH7iJiIieJ9jfjJmZmTA3N1dpNzU1\nBfDswdOq2LBhA9TU1ODj46Nou337NgDg448/hoeHByZPnowbN27gxx9/REpKCvbt2wc1NdbeEgnN\nSF8Lg7u3Rb+urRGTmIHIuBTsOHkdIWeT0c3BAl6uVmhu0uTVFyIiImoEBEvgCwsLoaGhultj2QOo\nRUVFlb5WeHg4QkJCMHnyZFhbWyvaHz9+DABwdHTE119/DQDw9fWFgYEBFi9ejNOnTytKbyrrZfVI\nNc3UlJviUM2pK+Orf7Om6O/ZDtfvZOPI77fw66VUnIpLhXM7E/Tt1gZuHcyhxjXl6526Mr6oYeL4\nosZGsAReS0sLJSUlKu1lifvzK8m8TExMDObOnYuePXtixowZKvcAAH9/f6X2/v37Y/HixYiLi6ty\nAs+HWKkhqovjy1BbHe95t0P/ri3xW3waTl9Kxf9+joaxviZ6uliiu3Nz6OlIhA6TKqEuji9qODi+\nqCGqsw+xmpqallsmk5mZCQCVqn9PTEzE1KlTIZVKERwcrFIOU1aOY2xsrNSup6cHiUSC3Nzc1w2f\niGqJvo4Efbu0gp+7Nf78Owun4lKw/+xNHDz3D9w7mKFXJyu0stAXOkwiIqJaI1gCb2dnh23btqGg\noEDpQdb4+HjF8Ze5c+cOgoKCYGRkhHXr1kFHR0elj729PQAgPT1dqf3hw4coLi7mQ6xE9YiaWIxO\nUlN0kpoiNTMfp+JS8ceV+/j9r/to21wfXp2s0FlqBg11ltcQEVHDJtjfdH5+figpKcG+ffsUbcXF\nxQgNDYWrq6viAde0tDSlpSGBZ7P0gYGBEIlE2LRpU4WJuLu7OwwNDREaGgqZTKZoL7vnq3Z8JaK6\nydJUF2N8pfj6w24Y6d0O+U9KsCH8Gj774XeE/noTD3MLhQ6RiIioxgi2kRMAzJgxA5GRkRg3bhys\nra0RFhaGK1euYMuWLejUqRMAYMyYMYiOjkZSUpLivAEDBiAxMRFBQUGwtbVVuqa1tbXSLq4hISGY\nO3cuunbtCm9vbyQnJ2PXrl3o3r071q1bV+WYWQNPDVF9H18yuRzX/nmIU7GpiL/xACKRCK5SU/Ry\ntYRtCwOuKS+w+j6+qG7j+KKGqM7WwAPA8uXLsWrVKhw8eBA5OTmQSqVYv369InmvSGJiIgBg48aN\nKscGDRqklMAHBARAUtFvxgAAIABJREFUQ0MDGzduxNKlS2FgYIBx48bh448/rt4XQ0SCEYtEcGht\nDIfWxsh89ASn41Lx2+U0xCRmwMq0Cbw6WaFLBwtoSrhsLBER1X+CzsDXR5yBp4aoIY6vopKniLqW\njlOxKbiTkQ9tTXW849QMnq6WMDdUfWaGak5DHF9Ud3B8UUNUp2fgiYhqiqaGGro7N8c7Ts1wIzUH\nkbEpiIxNwYmLd+HYxhi9OlnCoY0xxCyvISKieoYJPBE1aCKRCO2sDNDOygCP8otw9s80nLmUilX7\nLsPMQBterpbo5tQMTbRUN5YjIiKqi1hCU0UsoaGGqLGNr9KnMsRdz8QvsSm4kZIDiYYYXewt4OVq\nhRZmwu223FA1tvFFtYvjixqiWimhKS0tRWRkJHJycuDp6anYQImIqC5SVxPDrb053Nqb4/b9PJyK\nS8EfV+7j7J9psG1hgF6drODSzgTqalxTnoiI6p4qz8AvX74cUVFR2L9/PwBALpdj7NixiImJgVwu\nh4GBAfbu3Qtra+saCVhonIGnhojjC8h/UoJzl+/hVFwKHuQUwlBPEz06NkcP5+ZoqqspdHj1GscX\n1SSOL2qIXjUDX+Xppd9++w2dO3dW/Hzq1ClcvHgREyZMwNdffw0AWL9+/WuESkQkHF1tDfi5W2PZ\n5P/H3p2HN3ne6aO/tcuSvFvyLm+AzWJjm4ADhCXgpCQhYW+bTLO0hxkaklMapqeZNJ1reqZzmqZJ\nmqSZtkkJ6S+hJGnCTqGEAGELCZuNzWID3m28SN7kXZIlnT9sBMY2tkGypNf357p6gV/plR41X17f\nfvx9n2cmfrIyDVFhauw4Voqf/ekE/rL7IoqvmcCOQyIi8gYjbqGpra1FXFyc8+uvvvoKMTEx+NnP\nfgYAuHr1Knbv3u26ERIRjSKxWIT0cWFIHxeGmoZ2fJVzDcfP1+Dbi3WIi/DHwswYzJiog1zGNeWJ\niMgzRhzgrVYrpNIbp508eRKzZs1yfh0bGwuj0eia0REReVBkqBpPPDABy+Ym4tuLtTiYcw0f7C3A\nZ18VYc7USNyfEY2wQD9PD5OIiMaYEbfQREREIDc3F0DPbHtlZSWmT5/ufLyhoQEqFTdJISLh8FNI\ncX9mDH79f83A//N4BpJjg7DvZAVefPcbvLM1H5fKGtleQ0REo2bEM/CPPPII/vSnP6GxsRFXr16F\nRqPBvHnznI8XFBQI9gZWIhrbRCIRJsYFY2JcMBpMXTh87hqOnKtG7tV6RIaqsCAzBrOmRMBPwS02\niIjIfUb8XWbNmjWoqanBwYMHodFo8OqrryIgIAAA0NraikOHDuGZZ55x9TiJiLxKaKASK+Yl4bHZ\n8ThVYMDBs1XY/OUVbD1SjNlTIrFgWjQiQ9WeHiYREQmQSzdystvtaG9vh1KphEwmzF0NuYwkCRHr\nyzVKqltw8GwVThfWodvmwKT4YCzMjMHUcWEQi0WeHp7HsL7InVhfJESjspHTdd3d3fD393flSxIR\n+YzEqAAkRk3C9xaMw5G8ahzOvYZ3tp1HaIASCzKjMWdqFDR+wpzcICKi0TPim1iPHDmCd955p8+x\nzZs3IzMzE+np6fj3f/93WK1Wlw2QiMjXBKjleHRWPH737EysXToF2iAlPj9cjH//49f4YE8Byms5\nW0hERHduxDPwGzduRGhoqPPr4uJi/OY3v0FsbCxiYmKwd+9epKamsg+eiMY8iViMe1J0uCdFhypj\nGw7lXMOJCzU4fr4GSdEBWJgZg3tSdJBKRjyXQkREY9iIv2uUlJRgypQpzq/37t0LhUKBLVu24P33\n38fDDz+MHTt2uHSQRES+LkarwVPfScbvn5uN7y8cj9YOK/6y+xJ+9qcT2HGsBE2tZk8PkYiIfMSI\nZ+BNJhOCg4OdX584cQL33nsvNJqeRvsZM2bgyJEjrhshEZGAqJQyPDg9Ftn3xOBiaSMOnq3C7q/L\nsOebcmRO0GLhtBiMjwmESDR2b3olIqLbG3GADw4ORnV1NQCgra0N58+fx/r1652Pd3d3w2azuW6E\nREQCJBaJkJoYitTEUBiaOvBV7jUcy6vB6UIDYrQaLJwWjXsnRUAhl3h6qERE5GVGHODT09Px6aef\nYty4cTh69ChsNhvmzp3rfLy8vBw6nc6lgyQiEjJdsArfWzAeS+ck4uSlOhw4U4UP913G518V4760\nSCzIjIYumDtcExFRjxEH+J/85Cd46qmn8NOf/hQAsGzZMowbNw4A4HA4cODAAWRlZbl2lEREY4BC\nJsHcqVGYkxaJq1UmHDxbhYNnq/Dl6UqkJoVi4bQYTE4IgZjtNUREY9odbeTU3NyMnJwc+Pv7Y/r0\n6c7jJpMJO3bsQFZWFlJSUlw6UG/BjZxIiFhf3qup1Ywj567h8LlqtLRboAv2w4LMGNyXGgGV0jfW\nlGd9kTuxvkiIhtrIyaU7sY4FDPAkRKwv79dts+PMZQMOnb2GomsmyGVizJocgQWZMYjRDX6R9was\nL3In1hcJkdt2Yq2oqMDBgwdRWVkJAIiNjcXChQuh1+vv9CWJiGgQUokY906KwL2TIlBe24qDZ6tw\n/HwtDp+rRnJsEBZOi0H6+DCuKU9ENAbc0Qz8W2+9hQ0bNvRbbUYsFmPNmjVYt26dywbobTgDT0LE\n+vJNbZ1WHMurxqGca2ho6UKwvwLz06MwLz0aAWq5p4fnxPoid2J9kRC5fAZ+y5YtePfdd5GRkYHV\nq1dj/PjxAICrV69i48aNePfddxEbG4vly5ff+aiJiGhIGj8ZHro3Dt+ZoUdecT0Ona3C9mOl2H2i\nDNNTdFgwLQaJkQFcU56ISGBGPAO/fPlyyGQybN68GVJp3/zf3d2Nf/mXf4HVasW2bduGfC2LxYK3\n334bO3fuREtLC1JSUvDCCy9g5syZtz1v//792Lt3L/Lz89HQ0IDIyEjcf//9WLt2Lfz9/fs8Nzk5\necDX+NWvfoXHH398yDHeijPwJESsL+GoaWjHoZxr+Pp8DbosNsRH+GPhtBjMmKiDTOqZNeVZX+RO\nrC8SIpffxDp16lSsX78eTz/99ICPf/jhh/j973+PvLy8IV9r/fr12L9/P5566inExcVh+/btuHDh\nAjZt2oSMjIxBz8vKyoJOp0N2djaioqJw+fJlfPrpp4iPj8fWrVuhUCicz01OTsZ9992Hxx57rN/n\niI+PH96HvsloB/hTtTnYVbwPzeZmBCmC8FjSIsyIyBy196exgd8AhafT3I1vLtbi4Nkq1DR0QOMn\nw7z0KMxPj0ZooHJUx8L6IndifZEQubyFRiaToaOjY9DH29vbIZMNvbRZfn4+9uzZg5deegnPPPMM\nAGDp0qVYvHgxXn/9dWzevHnQc//whz/0W2t+ypQpePHFF7Fnz55+7TuJiYlYsmTJkGPyNqdqc/Bx\n4VZY7VYAQJO5GR8XbgUAhngiui0/hRQLMmNwf0Y0CsqbcPBsFfZ+W46935YjY7wWCzOjkRIXzPYa\nIiIfNOLlClJTU/H3v/8d9fX1/R5raGjAZ599hqlTpw75Ovv27YNMJsOqVaucxxQKBVauXImzZ8/C\nYDAMeu5AG0VlZ2cDAIqLiwc8p6urC2azechxeZNdxfuc4f06q92KXcX7PDQiIvI1IpEIk+JD8H+v\nSMOrP56Jh7LicKWyGa99eg6/fP8kDuVUodPc7elhEhHRCIx4Bn7t2rV45pln8PDDD2PFihXOXViL\nioqwbds2tLe34/XXXx/ydQoKCpCQkAC1Wt3neFpaGhwOBwoKCqDT6YY9rus/UAQHB/d7bMuWLdi0\naRMcDgcmTJiAn/zkJ3jggQeG/dqe0mRuHvT462f+iKSgeIwLSkBCYBw0MvWAzyUiui4s0A8r5ydh\nyX3xOFVgwIGzVfjb/ivYcrgYs1MjsSAzGpGhvJYQEXm7EQf46dOn45133sGvf/1r/PWvf+3zWFRU\nFF599VXcc889Q76O0WhEeHh4v+NarRYAbjsDP5ANGzZAIpHgwQcf7HM8IyMDDz/8MGJiYlBTU4OP\nPvoIzz//PN544w0sXrx4RO8x2oIVQQOGeKVEAZEI+KryOA5UHAEARKjDMS4wHklBCUgKTECIMoi/\nGieiAcmkEsxOjcSsKREoqW7BwZwqHM69hoNnqzA5PhgLp8UiLSkUYjGvIURE3uiOd2K12+24cOEC\nqqqqAPRs5DR58mR89tln+Oijj7B3797bnp+dnY1x48bh3Xff7XO8srIS2dnZ+M///E/84Ac/GNZY\ndu/ejZ/97GdYs2YN1q9ff9vndnR0YPHixbDZbDh8+LBXh9xj5afw3unNsNgszmNyiRxrpv8L5sTN\ngKXbgqLGchTWF6HQWITLDSXotHYBAEL9gpGsTcLEsHFI0SYhNjAKYhE3eCGigTW1dmH/t+XYe6IM\njS1d0IWo8MiseGTPiPOqNeWJiOgudmIVi8VIS0tDWlpan+NNTU0oLS0d8nylUgmr1drv+PU+9ZtX\nkrmdM2fO4OWXX8b8+fOHtYGUSqXC97//fbzxxhsoKSlBUlLSsN7nutFchSZFNRGPJy/vtwpNimqi\n8457rSgCWm0E5mjvg91hR3VbLYpMpShpLsPF2is4UXEGAOAnVSIxMB7jAhOQGBSPOP8YyCRD32xM\nYwNXcSAAWJAehbmpETh3tR4Hzlbhr/+4hL/tK0TWpHAszIxBXIT/0C8yANYXuRPri4TI5avQuIpW\nqx2wTcZoNALAsPrfCwsL8eyzzyI5ORlvvvkmJJLhrXEcGRkJADCZTCMYsWfMiMjEjIjMYV2gxCIx\nYvyjEOMfhfkxs+FwONDQ1YTi5lIUm0pR3FyGiw2FAACpWIo4/5jelpt4JAbGQyXzG42PREReTCoR\n454UHe5J0aHS0IZDOVX45mItjufXYFx0IBZOi8G0ZC2kEv5Gj4jIUzwW4FNSUrBp0ya0t7f3uZH1\n+vrxKSkptz2/oqICq1evRkhICN577z2oVKphv3dlZSUAICQk5A5G7jtEIhHC/EIQ5heCrMhpAIBW\nSxtKTOW9ob4MByqOYL/jK4ggQqQ6HON6A31SUAKClUEe/gRE5EmxOg2eXpSClfOT8HV+DQ7lXMN7\nuy4iUC3HvPQozEuPRrD/8H5bSkREruOxAL9o0SJ88MEH+Pzzz53rwFssFmzbtg2ZmZnOG1yrq6vR\n2dnZp9XFaDTiRz/6EUQiETZu3DhoEG9sbOz3WFNTEz7++GPExMTc0UZOvs5frsFU7WRM1U4GAFhs\nFpS1VKC4uQxFzaU4WXsWR699AwAIUQY7w3xSYDwi1Dr20RONQWqlDA/O0CN7eiwulDTiUE4Vdn1d\nhj3flGNashYLMmMwPibQq+8pIiISEo8F+KlTp2LRokV4/fXXYTQaodfrsX37dlRXV+OVV15xPu/F\nF1/EqVOncPnyZeex1atXo7KyEqtXr8bZs2dx9uxZ52N6vd65i+vmzZtx8OBBzJ8/H1FRUairq8Pf\n//53NDY24o9//OPofVgvJpfIMSF4HCYE9ywHarPbcK29BsXNZShuLkVh01WcrssFAKilKiQGxSEp\nMAFJQQnQ+0dDKvZYCRHRKBOLREhLCkVaUijqmjrwVc41HMuvwakCA2J1GiycFoOsSeFQyIbXzkhE\nRHdmWKvQ3Lpc5O2cOHECx48fR0FBwZDPNZvNeOutt7B7926YTCYkJydj/fr1mDVrlvM5Tz75ZL8A\nn5ycPOhrLlu2DL/97W8BAMePH8fGjRtx5coVmEwmqFQqpKenY82aNZg2bdqwP9PNRvMm1pt56iYd\nh8MBY2eDs+Wm2FQKQ0fPmvsysRTxAXrnLH1CYBz8pKO7RTu5Bm8CoztlttjwzaVaHDpbhSpjO9RK\nKe5Li8T9mTEovmbCtiPFaGwxIyRAgeXzkjBzcoSnh0wCw+sXCdFQN7EOK8AP1Y/e70VFomEFeF80\n1gL8QFosrShpLkNR742xVW3VsDvsEEGEGE0kEntbbsYFJSBQEeDp4dIweFN9kW9yOBy4UtmMgznX\nkHPZCLvDAZEIuPk7jFwqxtMPpTDEk0vx+kVC5JIAf+rUqRG/8YwZM0Z8ji9ggO+vq9vc20dfiiJT\nGcpM5bDYe5YIDVOG9PTQB8UjKTAB4Sot+2S9kDfXF/mexpYu/OfGk+g02/o9FhqgwGtrZ3tgVCRU\nvH6RELlkGUmhhnFyDaVUgZSQ8UgJGQ+gp4++qq0aRb1tNxcbCnGytuc+BY1MfePG2KB4xGqiIRGz\nX5ZISEIClAOGdwBoaDGP8miIiISHdyCSy0nEEsQFxCIuIBYLMRcOhwOGDqOz5abYVIa8+osAALlY\nhvjAOGfLTXyAHkopl6Uj8nWhAYpBw/prn+RixbwkJEaxxY6I6E4Mq4WGbmALjWs0m00oMZWjqLkU\nJc2lqGqrgQOOns2oNFHOlpukoHgEyO9s90caPqHVF3neNxdr8eE/C2HptjuPyaViTJugxYWyRrR2\nWDFtghbL5iYiKkx9m1ciuj1ev0iIXNIDTzcwwLtHZ3cXSk3lPSvdNJeirKUCVns3AEDnF+Zciz4p\nKB5avzD20buY0OuLPOObi7UDrkLTae7Gl6cr8c9TFbBYbZidGoml9yUgJICrWNHI8fpFQsQA72IM\n8KOj296NitZrzuUrS5rL0N7dAaBnM6qkwATnrrHRmkj20d+lsVZfNLoGq6+WDgv2flOOQzlVAERY\nkBmNR2bGwV8lH/1Bks/i9YuEiAHexRjgPcPusKOuw9hzY2xzGUpMpWjoagIAKCRyJATEISnoRh+9\nXMIAMBJjvb7IvYaqr3pTJ3YdL8PXF2qgkEmwKEuPB6fHQinnbVo0NF6/SIgY4F2MAd57NHU1O1tu\nik1lqG6rdfbR6/1jnC03SYEJ0MjZY3s7rC9yp+HW17X6dmw/WoKcK0b4q2R4dFY85qVHQyYVj8Io\nyVfx+kVCxADvYgzw3qvD2oGSm/roy1sq0e3oWcouXKXDOOeNsQkIVQazj/4mrC9yp5HWV3G1CVsP\nF6OwohlhgUosnZOAeydFQCzmv1nqj9cvEiIGeBdjgPcdVpsV5a1VKGkuQ7GpFMWmcnR2dwIAAuUB\nPbPzQQlICkxAtCYCYtHYneVjfZE73Ul9ORwOXCxrxNbDJSiva0W0Vo3lcxORPo43sVNfvH6REDHA\nuxgDvO+yO+yoaa9zttwUNZei2WwCACglSiQGxjlbbuICYiGXyDw84tHD+iJ3upv6sjscOFNowPaj\nJahr6kRSdABWzktCsj7YxaMkX8XrFwkRA7yLMcALS0NnU8/sfG+or2mvAwBIRBLEBcQ416JPDIyH\nWqby8Gjdh/VF7uSK+uq22fH1+RrsPF6K5jYLUhNDsWJeIvTh3CdirOP1i4SIAd7FGOCFrc3a3rMe\nfXPPDH1FaxVsvX30kepw53r044ISEKIUzgwg64vcyZX1ZbHacDCnCnu/KUd7VzeyJoVj6ZwEhAcL\n9wdsuj1ev0iIGOBdjAF+bLHYrChvqXC23JSaytFl69kePlgR1GfH2Eh1uM/20bO+yJ3cUV8dXVb8\n82QFvjxTCZvNgblTo/Do7HgEaRQufR/yfrx+kRAxwLsYA/zYZnfYca2ttrflpqf1xmTp+e/iJ/VD\nUmCcc6UbfUAMZGLfWMea9UXu5M76am4zY/eJMhw9Vw2JWIQHpsfioSw9VMqxcw/LWMfrFwkRA7yL\nMcDTzRwOBxq6Gp0tN8WmMtR1GAAAUrEUcf6xzg2mEgLioJL5eXjEA2N9kTuNRn0Zmjqw41gpvr1U\nB7VSiofvjcOCaTFQyLhLs9Dx+kVCxADvYgzwNJRWSxtKTGU9od5UisrWa7A77BBBhChNhLPlZlxQ\nAoIUgZ4eLgDWF7nXaNZXRV0rth0tQX5xA4I0cjx2XwLuS42EVOKb7W00NF6/SIgY4F2MAZ5Gymyz\noMxU0dtyU4aSlnJYbBYAQKgyGImBCT2bTAUlIEKl88ga16wvcidP1NeVymZsOVKMoioTdMF+WD43\nEfek6CDmGvKCw+sXCREDvIsxwNPdstltuNZWg6LeQF/cXIpWaxsAQC1TITEw3rnSTax/NKSj0EfP\n+iJ38lR9ORwO5BU3YOuRYlwztkMfrsHKeUmYnBDCzaAEhNcvEiIGeBdjgCdXczgcMHbWO1tuSprL\nYOisBwDIxDLEB8QiKSgB4wITEB+oh59U6fIxsL7InTxdX3a7Aycv1WH7sRLUm7qQog/CinlJSIr2\njhY2ujueri8id2CAdzEGeBoNJnNrbx99z2o3la3VcMABEUSI0UT2rEffuyZ9oCLgrt+P9UXu5C31\n1W2z48i5auz+uhQtHVZkjA/D8rmJiNYO/k2SvJ+31BeRKzHAuxgDPHlCV3cXSlsqnC03pS0VsNqt\nAIAwv1CM670xNikwHjqVdsTtAawvcidvq68uSze+PF2Jfacq0GWxYdaUCCy5LwFhgd65ShTdnrfV\nF5ErMMC7GAM8eQOb3YbKtmsoau5puSk2laHN2g4A0MjUvS03PTfGxmiiIBHffik91he5k7fWV2uH\nBXu/LcfBs9cAODA/IxqLZ8YjQC339NBoBLy1vojuBgO8izHAkzdyOByo6zA6V7opbi5FfVcjAEAu\nkSMhQI+k3kAfH6CHUtqzW+Wp2hzsKt6HZnMzghRBeCxpEWZEZHryo5AAefv1q7GlC7u+LsWx/BrI\nZRJ8Z3osvjNDDz+Fb2zENtZ5e30R3QkGeBdjgCdf0Ww29YT53lB/ra0GDjggFokRo4mCRqbGlaYi\ndDtsznNkYhmeSFnBEE8u5SvXr5qGdmw/WoIzl43Q+MmweFY87s+IgkzKzaC8ma/UF9FIeHWAt1gs\nePvtt7Fz5060tLQgJSUFL7zwAmbOnHnb8/bv34+9e/ciPz8fDQ0NiIyMxP3334+1a9fC399/0PPy\n8vLwve99Dw6HA6dPn0ZAwMhv/mOAJ1/V2d2JElMFSppLUWQqRVFz6YDPC1YE4X9m/2KUR0dC5mvX\nr9KaFmw9UoxLZU0IDVBgyX2JmDUlAmIxl570Rr5WX0TD4dUBfv369di/fz+eeuopxMXFYfv27bhw\n4QI2bdqEjIyMQc/LysqCTqdDdnY2oqKicPnyZXz66aeIj4/H1q1boVAo+p3jcDjw3e9+F0VFRejo\n6GCApzHvuUM/H/Sxh+OzkaFLQ6Q6nOtl013z1evXxbJGbD1cjLLaVkSFqbF8biIyxofx34SX8dX6\nIrqdoQK8xxr88vPzsWfPHrz00kt45plnAABLly7F4sWL8frrr2Pz5s2DnvuHP/wBWVlZfY5NmTIF\nL774Ivbs2YPly5f3O2f79u2oqKjAihUrsGnTJpd+FiJfFKwIQpO5ud9xqUiKf5YdxN6yAwhX6ZCp\nS0WGLg1R6ggGFxpTJseHYNLTwTh72YhtR0vwv9vOIzEqACvnJSElLtjTwyOiMcxjAX7fvn2QyWRY\ntWqV85hCocDKlSvx5ptvwmAwQKfTDXjureEdALKzswEAxcXF/R5ra2vD73//ezz//PNobu4fWIjG\noseSFuHjwq3O5SiBGz3wycHjkWc8jxxDPvaVHcI/yw5CpwpDpjYNGbo0RGsiGeZpTBCJRLgnRYeM\nCWE4cb4WO46X4nef5GJyQghWzEtEfMTd78NARDRSHgvwBQUFSEhIgFqt7nM8LS0NDocDBQUFgwb4\ngdTX9+xcGRzcf1bkT3/6EzQaDR5//HH8+c9/vruBEwnE9RtVB1uFZm7MLMyNmYUWSyvyjBeQYziP\nL8q/wr7yQ9D5hSFdl4pMXRpiNFEM8yR4ErEYc6ZG4d7J4TiUcw3/OFGG//4/ZzA9RYdlcxMREaLy\n9BCJaAzxWIA3Go0IDw/vd1yr1QIADAbDiF5vw4YNkEgkePDBB/scLysrw0cffYR33nkHUimXBCO6\n2YyITMyIyLxtD2mA3B9zomdiTvRMtFrakGe8gFzDeRyoOIL95V8hzC8Umbo0ZGhTEesfzTBPgiaT\nSvCdGXrMSYvCF6cqsP90Jc5eNmLO1Eg8NjsBwf7978EiInI1jyXarq4uyGSyfsev34BqNpuH/Vq7\nd+/Gli1bsGbNGuj1+j6PvfLKK5g+fTruv//+uxtwr9vdUOBuWu3gK+wQ3a3h1JcW/kiMjsQyPIAW\ncxtOVZ3DyaocZ5gPV4chKzYTM2MzkRisZ5gnJyFev/4tNhirHkzGZweuYN83ZfjmQi0enZOIFQvG\nw1/FzaBGkxDri+h2PBbglUolrFZrv+PXg/tAK8kM5MyZM3j55Zcxf/58rFu3rs9jR48exbFjx7B9\n+/a7H3AvrkJDQnSn9TU1YCqmTpqKtvHtyDdeRI4hH/+4fAC7CvcjVBnsbLOJ849lmB/DhH79Wn5f\nAuZMicCOY6XY9lUR9p4ow8P36pE9LRYKOdeQdzeh1xeNTV67Co1Wqx2wTcZoNALAsPrfCwsL8eyz\nzyI5ORlvvvkmJJK+F8rXXnsNCxYsgFqtRlVVFQCgpaUFAFBdXY2urq4R9dkT0cA0MjVmRc3ArKgZ\naLd29IR5Yz6+qjyOgxVHEawIQkZvmI8P4Mw8CY82yA//+ugkPJSlx7ajJdh6pAQHzlTh0dnxmDs1\nClKJ2NNDJCIB8ViAT0lJwaZNm9De3t7nRta8vDzn47dTUVGB1atXIyQkBO+99x5Uqv43ENXU1ODK\nlSv48ssv+z22ZMkSTJ06FZ999tldfhIiuplapsLMqOmYGTUdHdYO5NdfQq4hH0eqTuBQ5TFnmM/Q\npSI+QA+xiMGGhCNGp8FPVqahqMqELYeL8Lf9V/DFqQosm5OIGZPCIeYPr0TkAh7byCkvLw/f/e53\n+6wDb7FYsHjxYoSGhuKTTz4B0DNT3tnZiaSkJOe5RqMRjz/+OMxmMz755BPExMQM+B6HDx9Gd3d3\nn2N79uzB3r178dprryEyMhLTp08f0bjZQkNCNBr11WHtxPn6S8g15qOg4Qq6HTYEKQKRoe1ZZz4h\nkGFeqMbq9cttlpzFAAAgAElEQVThcOB8SSO2HilGpaENsToNVsxLRGpiKH8L5UJjtb5I2Lx6J9Z1\n69bh4MGDePrpp6HX6507sX744YeYNm0aAODJJ5/EqVOncPnyZed5S5YsQWFhIVavXo0JEyb0eU29\nXn/bXVzfeecd/O///i93YiW6yWjXV2d3J87XFyDHkI+CxivotncjUB7g7JlPDIxjmBeQsX79sjsc\nOFVQh+1HS2Bs7sKEmECsmJ+E8TFBnh6aIIz1+iJh8toeeAD43e9+h7feegs7d+6EyWRCcnIy/vKX\nvzjD+2AKCwsBAO+//36/x5YtW3bbAE9Enucn9XMuYdnZ3YUL9QXINeTj6+qTOFL1NQLl/kjXpSJD\nm4qkoASGefJpYpEI906KwD3JOhzLq8aur8vwyt9ykD4uDMvnJiJG57nVzYjIN3l0Bt4XcQaehMhb\n6quruwsXGgqRa8jHxYZCWO3dCJD7I107BRm6NIxjmPdJ3lJf3sJsseHA2Urs/bYCXeZu3Ds5Akvn\nJEAb5Ofpofkk1hcJkVe30PgiBngSIm+sr65uMy42FCDXcB4XGgphtVvhL9Ngqm4KMrU9YV4i5hJ9\nvsAb68sbtHVa8c9vy3HgbBXsdgfmZ0Rj8ax4BKq5hvxIsL5IiBjgXYwBnoTI2+vLbLPgYkMhcgz5\nuFhfAIvdCo1MjanaKcjUpWF8UCLDvBfz9vrytKZWM3Z/XYqjeTWQScV4YHosFs3QQ6Xk7uHDwfoi\nIWKAdzEGeBIiX6ovi82Ciw2XkWvIx/mGAlhslt4wPxkZujRMCEpimPcyvlRfnlTb2IEdx0pwqsAA\ntVKKR2bGY+G0aMikrOfbYX2REDHAuxgDPAmRr9aXxWbBpYbLyDWex/n6SzDbLFDLVJga1hPmk4PH\nMcx7AV+tL08pr23F1iPFuFDaiGB/BZbcl4DZqRGQiHn/x0BYXyREDPAuxgBPQiSE+rLYrChovIwc\nQz4u1Begy2aGSuqHNO1kZPaGeamYLQmeIIT68oSC8iZsPVKMkuoWRISosHxuIqYla7mG/C1YXyRE\nDPAuxgBPQiS0+rLarChovIIcQ8/MfJetC35SP6SFTUKmLg0pIeMZ5keR0OprNDkcDuRercfWI8Wo\naehAQqQ/VsxLwqT4EE8PzWuwvkiIGOBdjAGehEjI9WW1d6Ow8QpyDeeRX38Rnd1d8JMqkRY2GRm6\nVKSETICMYd6thFxfo8Vud+DEhVrsPF6ChhYzJsUHY8W8JCREjnxDQqFhfZEQMcC7GAM8CdFYqS+r\nvRuXG68ix5CP/PpL6OzuhFKiRGrYJGTqUjExZAJkEpmnhyk4Y6W+RoO124avcqvxjxNlaOu0Ylqy\nFsvnJiIyVO3poXkM64uEiAHexRjgSYjGYn1127txuamoJ8wbL6KjuxNKiQJTwiYiU5eGiSHJkDPM\nu8RYrC936zR344tTFfjidCUsVhvuS43EkvsSEBKg9PTQRh3ri4SIAd7FGOBJiMZ6fdnsNlxuKkKu\nIR95xoto7+6AQiJHatgkZGhTMSk0hWH+Loz1+nKnlg4L9pwox1e5VQBEWDgtGo/MjIfGb+zUK+uL\nhIgB3sUY4EmIWF832Ow2XGkuRq4hH+eMF9Bu7YBcIkdq6ERk6NIwOTQZcgl3yhwJ1pf71Zs6sfN4\nKU5cqIVSLsGiGXo8MD0WSrnw7+9gfZEQMcC7GAM8CRHra2A2uw1Xm0uQY8hHnvEC2qztkItlmNzb\nZjM5NAUKhvkhsb5GzzVjG7YdLUHu1XoEqGR4dHYC5qVHQSoR7hryrC8SIgZ4F2OAJyFifQ3NZreh\nqLkUOcZ85BkuoNXaBplYhimhKcjQpWJy6EQopQpPD9Mrsb5GX/E1E7YcLsblymaEBSqxbE4isiaF\nQywW3hryrC8SIgZ4F2OAJyFifY2M3WFHUXMpcg35yDWeR6ulJ8xPDk1Ghi4NU0JToJSOvZsJB8P6\n8gyHw4GLpY3YcqQYFXVtiNGqsXxeEqYmhQpqMyjWFwkRA7yLMcCTELG+7pzdYUdxcxlyjfk4ZzgP\nk6UVMrEUk0J6w3zYRPiN8TDP+vIsu8OBM4UGbD9agrqmToyLCcTKeUmYEBvk6aG5BOuLhIgB3sUY\n4EmIWF+uYXfYUWIqR47hephvgVQsxcSQCcjUpSE1bCL8pH6eHuaoY315h26bHcfP12DX8VI0t1mQ\nlhSK5XMToQ/39/TQ7grri4SIAd7FGOBJiFhfrmd32FFqqnC22TSbTZCKJJgYOgEZ2jSkhk2CSjY2\nwjzry7uYrTYcOluFPd+Uo9PcjaxJ4Vg6JwG6YJWnh3ZHWF8kRAzwLsYAT0LE+nIvu8OOspYK5BrO\nI9dwHk3mZkhEEkwMGY8MXRrSwiYLOsyzvrxTe5cV+05W4MvTlbDZHZibHoVHZ8UjSONbN2OzvkiI\nGOBdjAGehIj1NXrsDjvKWyqRaziPHEO+M8wnh4xDpjYNadrJUMt8cyZ0MKwv79bcZsbur8twNK8a\nEokID9wTi4ey9FApfWMzKNYXCREDvIsxwJMQsb48w+FwoLy10tkz39DVBLFIjOTgccjU9YR5jUzt\n6WHeNdaXb6hr6sCOY6U4eakOaqUUD8+Mw8LMGMhlEk8P7bZYXyREDPAuxgBPQsT68jyHw4GK1irn\nzHxDV6MzzGdoUzFVOwUauW+GedaXb6moa8W2oyXIL25AkEaOJfcl4L60SEjE3rkZFOuLhIgB3sUY\n4EmIWF/exeFwoLL1GnKNPWG+vrMBYpEYE4KSkKHrCfP+8sEv7N6G9eWbLlc0YeuREhRdMyE82A/L\n5ibinhQdxF62hjzri4SIAd7FGOBJiFhf3svhcKCqrbr3Bth8GDrrIYII44OTkKlLRbo21evDPOvL\ndzkcDuQVNWDr0WJcM7YjLtwfK+YnYnJ8iNdsBsX6IiFigHcxBngSItaXb3A4HLjWVoNcQz5yjPkw\ndPSG+aDE3pn5VAQqvG9Nb9aX77PbHfj2Ui12HCtFvakLKfogrJifhKSoQE8PjfVFgsQA72IM8CRE\nrC/f43A4UN1e2xPmDedR12GACCKMC0pAui4VGdpUBCoCPD1MAKwvIbF223Hk3DXsPlGG1g4rMido\nsWxuIqLDPHd/BuuLhMirA7zFYsHbb7+NnTt3oqWlBSkpKXjhhRcwc+bM2563f/9+7N27F/n5+Who\naEBkZCTuv/9+rF27Fv7+N2afmpub8corryA/Px+1tbUQi8WIj4/Hk08+iSVLltzRr/8Y4EmIWF++\nzeFwoKa9rndm/jxq2+sgggiJgfHI1KUhXTcFQQrPzZSyvoSn09yNL89UYt/JCpitNsyeEokl9yUg\nNFA56mNhfZEQeXWAX79+Pfbv34+nnnoKcXFx2L59Oy5cuIBNmzYhIyNj0POysrKg0+mQnZ2NqKgo\nXL58GZ9++ini4+OxdetWKBQ9m1BUVVXh5z//Oe655x5ERkbCbrfjxIkTOHDgANauXYt169aNeMwM\n8CRErC9hqWmvQ44hH7mGfNT0hvmEwLieMK+dgmBl0KiOh/UlXK0dFuz9thwHz14D4MCCzBg8PDMO\nASr5qI2B9UVC5LUBPj8/H6tWrcJLL72EZ555BgBgNpuxePFi6HQ6bN68edBzT548iaysrD7HduzY\ngRdffBGvvPIKli9fftv3/vGPf4xTp07h7NmzI56FZ4AnIWJ9CVdte51zacrq9loAQGJgHDJ0acjQ\npo5KmGd9CV9jSxd2Hi/F8fM1UMgk+M4MPR6cHgs/hdTt7836IiEaKsC7/1/WIPbt2weZTIZVq1Y5\njykUCqxcuRJvvvkmDAYDdDrdgOfeGt4BIDs7GwBQXFw85HtHR0ejs7MTVqsVcvnozRIQEY22CHU4\nHkoIx0MJ2ahrNziXptx6dTe2Xt2NhAA9MnRpSNemItQv2NPDJR8VEqDEDx+eiO/M0GP7sRLsPF6K\ng2er8OiseMzPiIZM6p1ryBP5Ko8F+IKCAiQkJECt7nvjS1paGhwOBwoKCgYN8AOpr68HAAQH9/8G\nZDab0d7ejo6ODpw5cwbbtm3DtGnTGN6JaEwJV+uwSL0Qi+IXwtBhdC5Nua3oH9hW9A/EBcQis3dm\nPtQvxNPDJR8UFabGc8tSUVLdgq1HivHJwavYf7oCS+ckYubkCIjF3rH0JJGv81iANxqNCA8P73dc\nq9UCAAwGw4heb8OGDZBIJHjwwQf7Pfb555/j17/+tfPrmTNn4re//e0IR9zjdr/OcDet1vuWhyPh\nYH2NLVr4Y3JcIn6AJahtNeDbqlx8U3kW24v2YHvRHiQFx+He2EzMjM2EThN29+/H+hpTtFp/ZE2N\nxrkrBny45xI27inA/jNVeOrhiciaHOHyNeRZXzTWeCzAd3V1QSaT9Tt+/QZUs9k87NfavXs3tmzZ\ngjVr1kCv1/d7PDs7G4mJiWhqasLhw4dhNBrR2dl5R+NmDzwJEetrbJPAD7PDZmF22CzUdzY4e+Y3\n52/H5vzt0PtH9/bMp0GrCh3x67O+xq7oYD+89C+ZOHvZiG1HS/D//fUUkqICsHJ+EpL1rmnZYn2R\nEHltD7xSqYTVau13/Hpwvx7kh3LmzBm8/PLLmD9//qCrykRERCAiIgIA8Mgjj+BXv/oVfvjDH2Lf\nvn1QKkd/ySsiIm8V5heKB+Lm44G4+WjobHT2zO8s/id2Fv8TsZqonjCvS4NOdfcz8yR8IpEI96To\nkDEhDF+fr8XO46V49eNcTEkIwYp5SYiL4Ow50Uh5LMBrtdoB22SMRiMADKv/vbCwEM8++yySk5Px\n5ptvQiKRDOu9v/Od7+CTTz7B6dOnMWfOnJENnIhojAj1C0G2fh6y9fPQ0NmEc8aenvldJfuwq2Qf\nojWRPT3zujSEq7SeHi55OYlYjLlTo3DvpHAcyrmGPd+U4f/9P6cxY6IOy+YkIjxE5ekhEvkMjwX4\nlJQUbNq0Ce3t7X1uZM3Ly3M+fjsVFRVYvXo1QkJC8N5770GlGv4//Ouz/K2t/JUbEdFwhPoFY6F+\nLhbq56KxqwnnDOeRYziP3SVfYHfJF4hSRzjDfIS6ZwLmVG0OdhXvQ7O5GUGKIDyWtAgzIjI9/EnI\n0+QyCRZl6TF3ahT2narA/tMVOFNoxNypkXh0dgKC/Yf3G3iiscxj68Dn5eXhu9/9bp914C0WCxYv\nXozQ0FB88sknAIDq6mp0dnYiKSnJea7RaMTjjz8Os9mMTz75BDExMQO+R2NjI0JC+q+k8Pzzz+PA\ngQP44osvEBcXN6JxsweehIj1RXeqqasZ54wXkGPIR4mpDAAQpY6Azi8MFxsLYbV3O58rE8vwRMoK\nhnjqw9Rmxj9OlOPwuWuQiEVYeE8MHr43Dmpl//vkBsLrFwmR127kBADr1q3DwYMH8fTTT0Ov1zt3\nYv3www8xbdo0AMCTTz6JU6dO4fLly87zlixZgsLCQqxevRoTJkzo85p6vd65i+s777yDAwcOYP78\n+YiOjobJZMKXX36JvLw8PPHEE/iv//qvEY+ZAZ6EiPVFrtBsNuGcoSfMF5tKB3yORqbG2qk/gkam\nhlqmhkIid/mKJOSbDM2d2HmsBN9erIOfQoqH7tUj+55YKGS3b4/l9YuEyKsDvNlsxltvvYXdu3fD\nZDIhOTkZ69evx6xZs5zPGSjAJycnD/qay5Ytcy4ReebMGfz1r3/FhQsX0NDQAJlMhuTkZKxcuRIr\nVqy4o28aDPAkRKwvcrXnDv18WM+TiqXQyNQ3/ifvCfb+vQFfI7/lMakKEvHw7nci31RpaMO2I8XI\nK25AoEaOx2YnYE5aJKSSgTeD4vWLhMirA7wvYoAnIWJ9kav98uvfoMnc3O94gFyDJ1JWos3Sjjbr\nTf+ztKPd2o5Wa8+fnd1dg762Suo3rLB//e8KiYKz/D7oalUzthwuxtUqE3RBflg6NwEzJoZDfMt/\nS16/SIgY4F2MAZ6EiPVFrnaqNgcfF26F1X5jueCR9MB327vRbu1whvuBwv7NX7dZ22Fz2AZ8LalI\nctuAr5ap4d/7w4BGpoFGxll+b+FwOHC+pAFbDpegytiGWJ0GK+YlITUxxPlDGa9fJEQM8C7GAE9C\nxPoidxjNVWgcDge6bGZnmL95Nv/GDwBtaLN0OB/r7B58Qz8/qR80MtUtM/0aqG86prke+OUqKCVK\nzvK7kd3hwKlLddh+rATG5i5MiA1Cij4IX5+vQWOLGSEBCiyfl4SZkyM8PVQil2CAdzEGeBIi1he5\nk7fWl81uQ5u1o3c2vw1t1o7esH/9721ot3ag1drzZ5ulDd2DzPJLRBJoZKremX5Nb/jv/bP3656Z\n/hs/BEjFHlvJ2Wd12+w4mleNLYeL0WXp+99CLhXj6YdSGOJJELx2J1YiIiJPkoglCFT4I1AxvJ1A\nHQ4HzDZzT7i3tvVp7bke8K8/VtVWjTZLOzpuM8uvlChvmsnvCfxquap3pl/dJ/xrZBr4STnLL5WI\nsSAzBnu/Ke8X4C3ddnx2qAj3Tgof8/8/kfAxwBMREQ2DSCSCUqqEUqpEmF//PUYGYrPb0NHdiVZL\n2yBtPT1/N5lbUNVWgzZrO7pvWjv/ZmKR+Ka+/evhvn9f/809/TKBzvI3tpoHPG5qt+Dnfz6B9HFa\npE8IQ3Js0KCr1xD5MmH+yyYiIvICErEE/nIN/OWD/yr8Zj2z/JYBb9K99Qbe6t7A32HthAMDt3Yq\nJYr+N/DeumSn8wZeNfykSohF3h94QwMUaGjpH+LVSin04f44ll+NgzlV8FNIkZoYgozxWqQmhkKl\nZOwhYWAlExEReYmeWX4FlFIFQkc4yz/Qcpw3h/8WSyuq22rRZm3vszrQzcQiMdRSVZ/Z/H7h/5av\nZZLh7ZjqSsvnJeHDfxbC0m13HpNLxXjigQmYOTkCZqsNBWVNyL1qRF5RPU4VGCARi5CiD0L6eC3S\nx4UhNFA56uMmchXexDpCvImVhIj1Re7E+vI+FpsFrbeG/QFm+6+v6NNu7Rh0ll8ukQ+8Fv9Am3PJ\n1VBJ/Vwyy//xma9wouEw7NJOiLv9MCt0Pp645/5+z7PbHSipbkHuVSPOFdWjpqEDAKAP1yB9XBgy\nxmuhD9ewb568ClehcTEGeBIi1he5E+vL99kddnRYO29Zj7/NuYpPq7O15/qNvO2w2CwDvpYIon7L\ncd4c8Aea6ZdL5H1e4272Gaht7EDuVSNyr9ajuMoEB3pactLHaZE+PgzJevbNk+cxwLsYAzwJEeuL\n3In1NTZZbJYby3BablqO86bwf+uSnYPO8otlffr1i5tLYRmgDShYEYT/mf2LYY+xpcOCvKJ6nLta\nj4uljbB029k3T16BAd7FGOBJiFhf5E6sLxoOu8OOzu6uW8L9za08N34YKG+tHPR1JoZMQKQ6HJHq\niN4/dVBKh+53v7VvvqXDColYhGR9EDLYN0+jjAHexRjgSYhYX+ROrC9ytV9+/Rs0mZv7HZeLZQhX\n61DbXgfrTctxhiiDEakOR9T1UK8JR4RK16815zr2zZOnMcC7GAM8CRHri9yJ9UWuNlQPvN1hR31n\nI2raa1HTXoea9jpUt9XC0GF07qYrggihfiHOUB+lDkekJgI6lbbf+vnsm6fRxgDvYgzwJESsL3In\n1he5w6naHOwq3odmczOCFEF4LGnRkDew2uw2GDsbUH092Lf1/GnorIfd0bMkpVgkhtYvrLf9JhxR\nmp6Ar/MLg0QsYd88jQoGeBdjgCchYn2RO7G+yJ1cUV9WezcMHcY+ob66vRb1nY3OG2slIgnCVdob\n/fWacIQptDDUipBX1MC+eXIpBngXY4AnIWJ9kTuxvsid3FlfFpsVdR0GVLfdaMWpaa9FQ1eT8zky\nsRThKh0i1eGQ24JgqpejvBww1AGAiH3zdEeGCvD8HQ8RERHRAOQSGWL9oxHrH93neFe3GbUddahp\nq3PO1l9tLkGz2dTzhDggIEEGFYLR3qbC3mIF/nFBg0BJKDLiY5ExXsu+eborDPBEREREI6CUKhAf\noEd8gL7P8Q5rpzPYX++zr5bWQqZqAwB0AThhk+J4vgaSMwGIVIdjcmQcZiaNR7h/EGfnadgY4ImI\niIhcQCXzQ2JgPBID4/scb7O2987W16KqtRYljddg7KpDjagCNU2nceAMILbLESILQ0JINBKCo3tW\nxVFHQCNXe+bDkFdjgCciIiJyI41MjfHBiRgfnOg85nA4YDK3IqeiBHlVZahorUaduBlGcy5OG087\nn+cv1zg3pYq6aYMqlczPEx+FvAQDPBEREdEoE4lECFIGYMGEdCyYkA6gZ735nCsGnC2pRIWpGiK/\nNnQEdKI60ISS5nJ0O26sex+kCHQudRmpjkCUJhwRqnAopQpPfSQaRQzwRERERF4gIkSFh++Nx8P3\nxvddbz63EZZuG/w0FsTFixCis0Dk1wZjlxHHrn3Tb9fZm2fqI3uDvVwi8+AnI1djgCciIiLyMgEq\nOeakRWFOWhTMVhsKypqQe9WIvKJ6FF6QQyL2R7J+IhaPC4U+VoJOcVOfXWcLGq/CdtOus2F+IT0z\n9ddn7QfZdZZ8A/+rEREREXkxhUyC9PFhSB8fBrvdgZLqFuQWGXHuaj0+PlAEANDrNEgfn4AF42dA\nP0kDu8MOY2c9qq+vX9+7lv2FhoJ+u87eHOqj1OHQ9u46S97Loxs5WSwWvP3229i5cydaWlqQkpKC\nF154ATNnzrztefv378fevXuRn5+PhoYGREZG4v7778fatWvh7+/vfF5NTQ22bNmCI0eOoLy8HGKx\nGBMmTMDatWuHfI/BcCMnEiLWF7kT64vcaazXV21jB3Kv9oT5oioTHABCAhTOzaNuXW/+TnadjdL0\nBPwwv1CIRVy7fjR49U6s69evx/79+/HUU08hLi4O27dvx4ULF7Bp0yZkZGQMel5WVhZ0Oh2ys7MR\nFRWFy5cv49NPP0V8fDy2bt0KhaLnBo6//e1veO2115CdnY3MzEx0d3dj586duHjxIl599VUsXbp0\nxGNmgCchYn2RO7G+yJ1YXzf06ZsvbYSl2w4/hQSpiaFIHx+GtMRQqJQD98L333W2589bd52NUOkQ\ncVOoj1JHIFgZxGDvYl4b4PPz87Fq1Sq89NJLeOaZZwAAZrMZixcvhk6nw+bNmwc99+TJk8jKyupz\nbMeOHXjxxRfxyiuvYPny5QCAq1evIjQ0FCEhIc7nWSwWLFmyBGazGYcOHRrxuBngSYhYX+ROrC9y\nJ9bXwCxWGy7d1Dff0mGFRCxCsj4IGeO1SB8XhtBA5ZCvc/Ous9c3p6ppr7ux6ywAuUSOSFXPDbPX\nQ32kOhxBikBuTnWHhgrwHuuB37dvH2QyGVatWuU8plAosHLlSrz55pswGAzQ6XQDnntreAeA7Oxs\nAEBxcbHz2Pjx4/s9Ty6XY968efjrX/+Krq4uKJVDFy8RERGRL5Hf2jdf0+Jstdn85RVs/vJKb998\nT6uNPlwzYNgeya6zFxsK8W3NGedz/KTKPktdXv8zQD7we9HweSzAFxQUICEhAWp13x3G0tLS4HA4\nUFBQMGiAH0h9fT0AIDg4eMjnGo1GqFQqZ6sNERERkVCJxSKMiw7EuOhArJo/rk/f/O6vy7Dr67Lb\n9s0PZDi7zl7vrz9nvICvq085n6OWqfrM1F+/gVYj466zw+WxAG80GhEeHt7vuFarBQAYDIYRvd6G\nDRsgkUjw4IMP3vZ55eXl+PLLL/HII4/wpz8iIiIacyJCVHgoKw4PZcX16Zs/nl+DQznXht03P5DB\ndp1ttbb1668/XZeLzu4u5/P85Zo+oT5K0/N3Pyl3nb2VxwJ8V1cXZLL+BXF9VtxsNg/7tXbv3o0t\nW7ZgzZo10Ov1gz6vs7MT69atg5+fH1544YWRDxq4bT+Su2m1/kM/iegOsb7InVhf5E6srzunBZAU\nF4rlC5NhttqQd8WIby/U4PSlOpwqMEAiFiE1KQxZUyIwY3IEdMGqO3ofHQKQhKg+xxwOBxo7m1Fp\nqkGlqRqVLdWoMtXgm9ozMHffyIGhfsGICYxEbEAkYgOjEBsYhZiACChlY7cN2mMBXqlUwmq19jt+\nPbgPt73lzJkzePnllzF//nysW7du0OfZbDa88MILKC4uxsaNG0fUnnMz3sRKQsT6IndifZE7sb5c\nK0GnRsKCcfje/KQ+ffPvbT+P97afH1bf/MhIES2NRXRoLO4N7Tlid9jR1NXsbMG5vuTlJcOVPrvO\nhiqD+/TXR2kiEK7SCWLXWa+9iVWr1Q7YJmM0GgFgWAG7sLAQzz77LJKTk/Hmm29CIhl804Ff/vKX\nOHLkCN544w3MmDHjzgdOREREJHDu6Jsf9nuLxAj1C0GoXwimhE10Hrc77KjvbLzRX9/bkjPQrrPO\nVpzeNpxwlRZSAe0667FPkpKSgk2bNqG9vb3Pjax5eXnOx2+noqICq1evRkhICN577z2oVIP/SufV\nV1/Ftm3b8Mtf/hIPP/ywaz4AERER0Rjhzr754RKLxNCpwqBThWGqdorzuM1uu7HrbNuNpS7P37Lr\nrM4vrE+oH2rX2VO1OdhVvA9N5mYEK4LwWNIizIjIdPnnuhMeC/CLFi3CBx98gM8//9y5DrzFYsG2\nbduQmZnpvMG1uroanZ2dSEpKcp5rNBrxox/9CCKRCBs3buyzzvut3n//fXzwwQf48Y9/jCeffNKt\nn4mIiIhI6AJUcsxJi8KctKh+681f75tP1gc5Z+eHs9783ZCIJYhQhyNCHQ7o0pzHB9p1tqqtGueM\nF5y7zkpFEuhUWucNs9dbckpN5fjk8jZY7T3t3k3mZnxcuBUAvCLEe3Qn1nXr1uHgwYN4+umnodfr\nnTuxfvjhh5g2bRoA4Mknn8SpU6dw+fJl53lLlixBYWEhVq9ejQkTJvR5Tb1e79zF9csvv8Tzzz+P\n+Ph4rF27tt/7P/DAA7eduR8Ie+BJiFhf5E6sL3In1pf3uHW9+ZqGDgBwQ9/83RnOrrODCVYE4X9m\n/8LtY4Aaw5MAAAxqSURBVPTaHngA+N3vfoe33noLO3fuhMlkQnJyMv7yl784w/tgCgsLAfTMrt9q\n2bJlzgB//XllZWX4+c9/3u+5Bw8eHHGAJyIiIqL+PNk3PxJyiQyx/tGI9Y/uc/zmXWf/Vvj5gOc2\nmZtHY4hD8ugMvC/iDDwJEeuL3In1Re7E+vINN/fNXyxthKXbPip983fql1//ZsCwzhl4IiIiIhoT\nvK1vfiiPJS3Cx4VbnT3wACATy/BY0iIPjuoGzsCPEGfgSYhYX+ROrC9yJ9aXb/PmvnlPrkIz1Aw8\nA/wIMcCTELG+yJ1YX+ROrC9hublvvqjKBAfgFX3zo40tNERERETkE7xhvXlfwABPRERERF7H1/rm\nRxNbaEaILTQkRKwvcifWF7kT62vs8ea+eVdhD7yLMcCTELG+yJ1YX+ROrC8SYt88e+CJiIiISLDG\nYt88AzwRERERCcJw+uYnxAYhY3wY0seHISzQz9NDviNsoRkhttCQELG+yJ1YX+ROrC8ajjvpm//m\nYi22HSlGQ4sZoQEKLJ+XhJmTI0ZlvOyBdzEGeBIi1he5E+uL3In1RXdiqL755jYzNn1xGZZuu/Mc\nuVSMpx9KGZUQzx54IiIiIqKbDNU3LwJw63StpduObUeKR20W/nYY4ImIiIhozBqob/4PW/MHfG5D\ni3mURzcw31pTh4iIiIjITeQyCdLHhyE0QDHg44MdH20M8EREREREN1k+Lwlyad+YLJeKsXxekodG\n1BdbaIiIiIiIbnK9z91Tq9AMhQGeiIiIiOgWMydHeE1gvxVbaIiIiIiIfAgDPBERERGRD2GAJyIi\nIiLyIQzwREREREQ+hAGeiIiIiMiHMMATEREREfkQBngiIiIiIh/CAE9ERERE5EMY4ImIiIiIfAh3\nYh0hsVg0Jt+bhI/1Re7E+iJ3Yn2R0AxV0yKHw+EYpbEQEREREdFdYgsNEREREZEPYYAnIiIiIvIh\nDPBERERERD6EAZ6IiIiIyIcwwBMRERER+RAGeCIiIiIiH8IAT0RERETkQxjgiYiIiIh8CAM8ERER\nEZEPYYAnIiIiIvIhUk8PgAZmMBjw0UcfIS8vDxcuXEBHRwc++ugjZGVleXpoJAD5+fnYvn07Tp48\nierqagQFBSEjIwM//elPERcX5+nhkY87f/483n33XVy6dAkNDQ3w9/dHSkoKnnvuOWRmZnp6eCRA\nGzZswOuvv46UlBTs3LnT08MhcjsGeC9VWlqKDRs2IC4uDsnJycjNzfX0kEhA3n//feTk5GDRokVI\nTk6G0WjE5s2bsXTpUmzZsgVJSUmeHiL5sMrKSthsNqxatQparRatra3YvXs3fvCDH2DDhg2YPXu2\np4dIAmI0GvHnP/8ZKpXK00MhGjUih8Ph8PQgqL+2tjZYrVYEBwfjwIEDeO655zgDTy6Tk5ODKVOm\nQC6XO4+VlZXh0UcfxSOPPILf/va3HhwdCVFnZyeys7MxZcoUvPfee54eDgnIf/zHf6C6uhoOhwMt\nLS2cgacxgT3wXkqj0SA4ONjTwyCByszM7BPeASA+Ph7jx49HcXGxh0ZFQubn54eQkBC0tLR4eigk\nIPn5+di1axdeeuklTw+FaFQxwBMRAMDhcKC+vp4/OJLLtLW1obGxESUlJfj973+PK1euYObMmZ4e\nFgmEw+HAr3/9ayxduhQTJ0709HCIRhV74IkIALBr1y7U1dXhhRde8PRQSCB+8Ytf4IsvvgAAyGQy\nfP/738ePf/xjD4+KhGLHjh0oKirCH//4R08PhWjUMcATEYqLi/Hf//3fmDZtGpYsWeLp4ZBAPPfc\nc/je976H2tpa7Ny5ExaLBVartV/7FtFItbW14Y033sC//du/QafTeXo4RKOOLTREY5zRaMSaNWsQ\nGBiIt99+G2IxLwvkGsnJyZg9ezZWrFiBjRs34uLFi+xVJpf485//DJlMhh/+8IeeHgqRR/A7NdEY\n1train/9139Fa2sr3n//fWi1Wk8PiQRKJpNh4cKF2L9/P7q6ujw9HPJhBoMBH374IZ544gnU19ej\nqqoKVVVVMJvNsFqtqKqqgslk8vQw6f9v7/5Cmvr/OI6/1MiglNAMQu2PBYo6nBf90VDMKUQYdhFI\n6Yq0QS0DC7spugiKgiyilWB5Ud3khQmDXVTWBKtBQZREJmFZOfrLTKI0K9334st3tN/89RW+6Dzu\n+bg77/Oee58h48XZ55yDScUSGiBCjYyMaNeuXXr16pUuXbqktLS0cI+EGe779+/y+/369u2b5syZ\nE+5xYFA+n08/f/5UQ0ODGhoaQvZbLBbZbDbV19eHYTpgahDggQg0Ojqquro6PX78WI2NjTKbzeEe\nCTPIwMCAEhISgmpfv37VjRs3tGjRIiUmJoZpMswEKSkp4164eubMGQ0NDengwYNaunTp1A8GTCEC\n/DTW2NgoSYH7cjudTj18+FDx8fGqqqoK52gwuBMnTsjtdmvdunUaHBwMevDJ3LlzVVJSEsbpYHR1\ndXWKjY1Vbm6ukpKS9O7dO7W1ten9+/c6ffp0uMeDwcXFxY37HXX58mXFxMTw/YWIwJNYp7H09PRx\n68nJyXK73VM8DWYSq9WqBw8ejLuP/y/8V62trXI6nert7dWXL18UFxcns9ms6upqrVq1KtzjYYay\nWq08iRURgwAPAAAAGAh3oQEAAAAMhAAPAAAAGAgBHgAAADAQAjwAAABgIAR4AAAAwEAI8AAAAICB\nEOABAAAAAyHAAwCmPavVquLi4nCPAQDTwqxwDwAACI/79+9r27Zt/3d/TEyMuru7p3AiAMBEEOAB\nIMKVlZWpsLAwpB4dzY+0ADAdEeABIMJlZmaqvLw83GMAACaI0ysAgD/yer1KT0+Xw+GQy+XSxo0b\nZTKZVFRUJIfDoV+/foW8pqenR3v27NHq1atlMpm0YcMGXbx4UaOjoyG9nz590tGjR2WxWJSdna28\nvDzt2LFD9+7dC+n98OGD9u/fr5UrVyonJ0c1NTXq6+ublOMGgOmKM/AAEOGGh4c1MDAQUp89e7bm\nzZsX2Ha73erv71dlZaUWLFggt9utc+fO6e3btzp+/Hig78mTJ7JarZo1a1agt6OjQw0NDerp6dGp\nU6cCvV6vV1u2bJHP51N5ebmys7M1PDysrq4ueTwerV27NtA7NDSkqqoq5eTkaN++ffJ6vbpy5Yrs\ndrtcLpdiYmIm6RMCgOmFAA8AEc7hcMjhcITUi4qK1NTUFNju6elRa2ursrKyJElVVVWqra1VW1ub\nKioqZDabJUnHjh3Tjx8/1NLSooyMjEBvXV2dXC6XNm/erLy8PEnSkSNH9PHjRzU3N6ugoCDo/cfG\nxoK2P3/+rJqaGtlstkAtISFBJ0+elMfjCXk9AMxUBHgAiHAVFRVav359SD0hISFoOz8/PxDeJSkq\nKko7d+7UrVu31N7eLrPZLJ/Pp0ePHqm0tDQQ3v/p3b17t65fv6729nbl5eVpcHBQd+7cUUFBwbjh\n+38voo2Ojg65a86aNWskSa9fvybAA4gYBHgAiHBLlixRfn7+v/YtX748pLZixQpJUn9/v6S/l8T8\nXv9dWlqaoqOjA71v3ryR3+9XZmbmhOZcuHChYmNjg2rz58+XJA0ODk7obwDATMBFrAAAQ/jTGne/\n3z+FkwBAeBHgAQAT8uLFi5Bab2+vJCk1NVWSlJKSElT/3cuXLzU2NhboXbx4saKiovTs2bPJGhkA\nZiQCPABgQjwej54+fRrY9vv9am5uliSVlJRIkhITE5Wbm6uOjg49f/48qPfChQuSpNLSUkl/L38p\nLCxUZ2enPB5PyPtxVh0AxscaeACIcN3d3XI6nePu+yeYS1JGRoa2b9+uyspKJSUl6fbt2/J4PCov\nL1dubm6g79ChQ7JaraqsrNTWrVuVlJSkjo4O3b17V2VlZYE70EjS4cOH1d3dLZvNpk2bNikrK0sj\nIyPq6upScnKyDhw4MHkHDgAGRYAHgAjncrnkcrnG3Xfz5s3A2vPi4mItW7ZMTU1N6uvrU2Jioux2\nu+x2e9BrTCaTWlpadPbsWV29elVDQ0NKTU1VfX29qqurg3pTU1N17do1nT9/Xp2dnXI6nYqPj1dG\nRoYqKiom54ABwOCi/PxGCQD4A6/XK4vFotraWu3duzfc4wBAxGMNPAAAAGAgBHgAAADAQAjwAAAA\ngIGwBh4AAAAwEM7AAwAAAAZCgAcAAAAMhAAPAAAAGAgBHgAAADAQAjwAAABgIAR4AAAAwED+AtNI\n4Zo513YoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdKwFWsQsZ-5",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hvIeuts3ysN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "f5b4d464-11d6-4225-f470-ada2299f83d4"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(test_dl)) * batch_size)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "total_accuracy_test = 0\n",
        "\n",
        "for batch in test_dl:\n",
        "    # add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    cur_input_ids, cur_input_mask, cur_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loss, logits = model(cur_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=cur_input_mask)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = cur_labels.to('cpu').numpy()\n",
        "\n",
        "    total_accuracy_test += get_accuracy(logits, label_ids)\n",
        "\n",
        "print('Testing Accuracy: {:.2f}'.format(total_accuracy_test/len(test_dl)))\n",
        "print('Testing Completed')\n",
        "\n",
        "\n",
        "### TO-DO: ROC-AUC curve -- good metric\n",
        "# https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...Predicting labels for 46 test sentences...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a030ca578ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         loss, logits = model(cur_input_ids, \n\u001b[1;32m     15\u001b[0m                         \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                         attention_mask=cur_input_mask)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Move logits and labels to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxDhqDy-se3y",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8kHkquMmVNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}